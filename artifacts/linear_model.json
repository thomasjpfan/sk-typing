{
  "ARDRegression": {
    "name": "sklearn.linear_model._bayes.ARDRegression",
    "common_name": "ARDRegression",
    "description": "Bayesian ARD regression. Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)  Read more in the :ref:`User Guide <bayesian_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "n_iter",
        "init_args": {
          "semantic_types": [
            "n_iter"
          ],
          "_structural_type": "int",
          "default": 300,
          "description": "Maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Stop the algorithm if w has converged."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_1",
        "init_args": {
          "semantic_types": [
            "alpha_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_2",
        "init_args": {
          "semantic_types": [
            "alpha_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_1",
        "init_args": {
          "semantic_types": [
            "lambda_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_2",
        "init_args": {
          "semantic_types": [
            "lambda_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "compute_score",
        "init_args": {
          "semantic_types": [
            "compute_score"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If True, compute the objective function at each step of the model."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "threshold_lambda",
        "init_args": {
          "semantic_types": [
            "threshold_lambda"
          ],
          "_structural_type": "float",
          "default": 10000.0,
          "description": "threshold for removing (pruning) weights with high precision from the computation."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Verbose mode when fitting the model."
        }
      }
    ]
  },
  "BayesianRidge": {
    "name": "sklearn.linear_model._bayes.BayesianRidge",
    "common_name": "BayesianRidge",
    "description": "Bayesian ridge regression. Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "n_iter",
        "init_args": {
          "semantic_types": [
            "n_iter"
          ],
          "_structural_type": "int",
          "default": 300,
          "description": "Maximum number of iterations. Should be greater than or equal to 1."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Stop the algorithm if w has converged."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_1",
        "init_args": {
          "semantic_types": [
            "alpha_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_2",
        "init_args": {
          "semantic_types": [
            "alpha_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_1",
        "init_args": {
          "semantic_types": [
            "lambda_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_2",
        "init_args": {
          "semantic_types": [
            "lambda_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_init",
        "init_args": {
          "semantic_types": [
            "alpha_init"
          ],
          "_structural_type": "float",
          "default": "None",
          "description": "Initial value for alpha (precision of the noise). If not set, alpha_init is 1/Var(y).      .. versionadded:: 0.22"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_init",
        "init_args": {
          "semantic_types": [
            "lambda_init"
          ],
          "_structural_type": "float",
          "default": "None",
          "description": "Initial value for lambda (precision of the weights). If not set, lambda_init is 1.      .. versionadded:: 0.22"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "compute_score",
        "init_args": {
          "semantic_types": [
            "compute_score"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If True, compute the log marginal likelihood at each iteration of the optimization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. The intercept is not treated as a probabilistic parameter and thus has no associated variance. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Verbose mode when fitting the model."
        }
      }
    ]
  },
  "ElasticNet": {
    "name": "sklearn.linear_model._coordinate_descent.ElasticNet",
    "common_name": "ElasticNet",
    "description": "Linear regression with combined L1 and L2 priors as regularizer. Minimizes the objective function::          1 / (2 * n_samples) * ||y - Xw||^2_2         + alpha * l1_ratio * ||w||_1         + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to::          a * L1 + b * L2  where::          alpha = a + b and l1_ratio = a / (a + b)  The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.  Read more in the :ref:`User Guide <elastic_net>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.5,
          "description": "The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If ``False``, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__bool",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, forces the coefficients to be positive."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ]
  },
  "ElasticNetCV": {
    "name": "sklearn.linear_model._coordinate_descent.ElasticNetCV",
    "common_name": "ElasticNetCV",
    "description": "Elastic Net model with iterative fitting along a regularization path. See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <elastic_net>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "name": "l1_ratio",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "default": "l1_ratio__float",
          "description": "float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For ``l1_ratio = 0`` the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]``"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__float",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "float",
              "default": 0.5
            }
          },
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__list",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "list"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path, used for each l1_ratio."
        }
      },
      {
        "name": "alphas",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "default": "alphas__None",
          "description": "List of alphas where to compute the models. If None alphas are set automatically"
        },
        "hyperparams": [
          {
            "name": "alphas__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alphas"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "alphas__None",
            "init_args": {
              "semantic_types": [
                "alphas"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "Amount of verbosity."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, forces the coefficients to be positive."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ]
  },
  "HuberRegressor": {
    "name": "sklearn.linear_model._huber.HuberRegressor",
    "common_name": "HuberRegressor",
    "description": "Linear regression model that is robust to outliers. The Huber Regressor optimizes the squared loss for the samples where ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.  This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.  Read more in the :ref:`User Guide <huber_regression>`  .. versionadded:: 0.18",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 1.35,
          "description": "The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations that ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Regularization parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not to fit the intercept. This can be set to False if the data is already centered around the origin."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 1e-05,
          "description": "The iteration will stop when ``max{|proj g_i | i = 1, ..., n}`` <= ``tol`` where pg_i is the i-th component of the projected gradient."
        }
      }
    ]
  },
  "Lars": {
    "name": "sklearn.linear_model._least_angle.Lars",
    "common_name": "Lars",
    "description": "Least Angle Regression model a.k.a. LAR Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "Sets the verbosity amount"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "n_nonzero_coefs",
        "init_args": {
          "semantic_types": [
            "n_nonzero_coefs"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Target number of non-zero coefficients. Use ``np.inf`` for no limit."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, ``np.finfo(np.float).eps`` is used."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_path",
        "init_args": {
          "semantic_types": [
            "fit_path"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha."
        }
      }
    ]
  },
  "LarsCV": {
    "name": "sklearn.linear_model._least_angle.LarsCV",
    "common_name": "LarsCV",
    "description": "Cross-validated Least Angle Regression model. See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Sets the verbosity amount"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_n_alphas",
        "init_args": {
          "semantic_types": [
            "max_n_alphas"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of points on the path used to compute the residuals in the cross-validation"
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. By default, ``np.finfo(np.float).eps`` is used."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      }
    ]
  },
  "Lasso": {
    "name": "sklearn.linear_model._coordinate_descent.Lasso",
    "common_name": "Lasso",
    "description": "Linear Model trained with L1 prior as regularizer (aka the Lasso) The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Technically the Lasso model is optimizing the same objective function as the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).  Read more in the :ref:`User Guide <lasso>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the L1 term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__bool",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, forces the coefficients to be positive."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ]
  },
  "LassoCV": {
    "name": "sklearn.linear_model._coordinate_descent.LassoCV",
    "common_name": "LassoCV",
    "description": "Lasso linear model with iterative fitting along a regularization path. See glossary entry for :term:`cross-validation estimator`.  The best model is selected by cross-validation.  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <lasso>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path"
        }
      },
      {
        "name": "alphas",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "default": "alphas__None",
          "description": "List of alphas where to compute the models. If ``None`` alphas are set automatically"
        },
        "hyperparams": [
          {
            "name": "alphas__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alphas"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "alphas__None",
            "init_args": {
              "semantic_types": [
                "alphas"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "Amount of verbosity."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_jobs",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "_structural_type": "int",
          "default": "None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If positive, restrict regression coefficients to be positive"
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ]
  },
  "LassoLars": {
    "name": "sklearn.linear_model._least_angle.LassoLars",
    "common_name": "LassoLars",
    "description": "Lasso model fit with Least Angle Regression a.k.a. Lars It is a Linear Model trained with an L1 prior as regularizer.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the penalty term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by :class:`LinearRegression`. For numerical reasons, using ``alpha = 0`` with the LassoLars object is not advised and you should prefer the LinearRegression object."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "Sets the verbosity amount"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, ``np.finfo(np.float).eps`` is used."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_path",
        "init_args": {
          "semantic_types": [
            "fit_path"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True`` the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator."
        }
      }
    ]
  },
  "LassoLarsCV": {
    "name": "sklearn.linear_model._least_angle.LassoLarsCV",
    "common_name": "LassoLarsCV",
    "description": "Cross-validated Lasso, using the LARS algorithm. See glossary entry for :term:`cross-validation estimator`.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "Sets the verbosity amount"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_n_alphas",
        "init_args": {
          "semantic_types": [
            "max_n_alphas"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of points on the path used to compute the residuals in the cross-validation"
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. By default, ``np.finfo(np.float).eps`` is used."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsCV only makes sense for problems where a sparse solution is expected and/or reached."
        }
      }
    ]
  },
  "LassoLarsIC": {
    "name": "sklearn.linear_model._least_angle.LassoLarsIC",
    "common_name": "LassoLarsIC",
    "description": "Lasso model fit with Lars using BIC or AIC for model selection The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Enumeration",
        "name": "criterion",
        "init_args": {
          "semantic_types": [
            "criterion"
          ],
          "values": [
            "bic",
            "aic"
          ],
          "_structural_type": "str",
          "default": "aic",
          "description": "The type of criterion to use."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "Sets the verbosity amount"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform. Can be used for early stopping."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, ``np.finfo(np.float).eps`` is used"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsIC only makes sense for problems where a sparse solution is expected and/or reached."
        }
      }
    ]
  },
  "LinearRegression": {
    "name": "sklearn.linear_model._base.LinearRegression",
    "common_name": "LinearRegression",
    "description": "Ordinary least squares Linear Regression. LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ]
  },
  "LogisticRegression": {
    "name": "sklearn.linear_model._logistic.LogisticRegression",
    "common_name": "LogisticRegression",
    "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr', and uses the cross-entropy loss if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs', 'sag', 'saga' and 'newton-cg' solvers.)  This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note that regularization is applied by default**. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver.  Read more in the :ref:`User Guide <logistic_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l1",
            "l2",
            "elasticnet",
            "none"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver. If 'none' (not supported by the liblinear solver), no regularization is applied.  .. versionadded:: 0.19    l1 penalty with SAGA solver (allowing 'multinomial' + L1)"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "dual",
        "init_args": {
          "semantic_types": [
            "dual"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Tolerance for stopping criteria."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "C",
        "init_args": {
          "semantic_types": [
            "C"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "intercept_scaling",
        "init_args": {
          "semantic_types": [
            "intercept_scaling"
          ],
          "_structural_type": "bool",
          "default": 1,
          "description": "Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  .. versionadded:: 0.17    *class_weight='balanced'*"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``solver`` == 'sag' or 'liblinear'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "newton-cg",
            "lbfgs",
            "liblinear",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "lbfgs",
          "description": "Algorithm to use in the optimization problem.  - For small datasets, 'liblinear' is a good choice, whereas 'sag' and   'saga' are faster for large ones. - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'   handle multinomial loss; 'liblinear' is limited to one-versus-rest   schemes. - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty - 'liblinear' and 'saga' also handle L1 penalty - 'saga' also supports 'elasticnet' penalty - 'liblinear' does not support setting ``penalty='none'``  Note that 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  .. versionadded:: 0.17    Stochastic Average Gradient descent solver. .. versionadded:: 0.19    SAGA solver. .. versionchanged:: 0.22     The default solver changed from 'liblinear' to 'lbfgs' in 0.22."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations taken for the solvers to converge."
        }
      },
      {
        "type": "Enumeration",
        "name": "multi_class",
        "init_args": {
          "semantic_types": [
            "multi_class"
          ],
          "values": [
            "auto",
            "ovr",
            "multinomial"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.  .. versionadded:: 0.18    Stochastic Average Gradient descent solver for 'multinomial' case. .. versionchanged:: 0.22     Default changed from 'ovr' to 'auto' in 0.22."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See :term:`the Glossary <warm_start>`.  .. versionadded:: 0.17    *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPU cores used when parallelizing over classes if multi_class='ovr'\". This parameter is ignored when the ``solver`` is set to 'liblinear' regardless of whether 'multi_class' is specified or not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "l1_ratio",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "default": "l1_ratio__None",
          "description": "The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'`. Setting ``l1_ratio=0`` is equivalent to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__float",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "l1_ratio__None",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ]
  },
  "LogisticRegressionCV": {
    "name": "sklearn.linear_model._logistic.LogisticRegressionCV",
    "common_name": "LogisticRegressionCV",
    "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for :term:`cross-validation estimator`.  This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. Elastic-Net penalty is only supported by the saga solver.  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter is selected by the cross-validator :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).  Read more in the :ref:`User Guide <logistic_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "name": "Cs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "Cs"
          ],
          "default": "Cs__int",
          "description": "Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "Cs__int",
            "init_args": {
              "semantic_types": [
                "Cs"
              ],
              "_structural_type": "int",
              "default": 10
            }
          },
          {
            "type": "Hyperparameter",
            "name": "Cs__list",
            "init_args": {
              "semantic_types": [
                "Cs"
              ],
              "_structural_type": "list"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module :mod:`sklearn.model_selection` module for the list of possible cross-validation objects.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "dual",
        "init_args": {
          "semantic_types": [
            "dual"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features."
        }
      },
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l1",
            "l2",
            "elasticnet"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver."
        }
      },
      {
        "name": "scoring",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "scoring"
          ],
          "default": "scoring__None",
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. For a list of scoring functions that can be used, look at :mod:`sklearn.metrics`. The default scoring option used is 'accuracy'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "scoring__str",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "scoring__Callable",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "scoring__None",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "newton-cg",
            "lbfgs",
            "liblinear",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "lbfgs",
          "description": "Algorithm to use in the optimization problem.  - For small datasets, 'liblinear' is a good choice, whereas 'sag' and   'saga' are faster for large ones. - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'   handle multinomial loss; 'liblinear' is limited to one-versus-rest   schemes. - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas   'liblinear' and 'saga' handle L1 penalty. - 'liblinear' might be slower in LogisticRegressionCV because it does   not handle warm-starting.  Note that 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  .. versionadded:: 0.17    Stochastic Average Gradient descent solver. .. versionadded:: 0.19    SAGA solver."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Tolerance for stopping criteria."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations of the optimization algorithm."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  .. versionadded:: 0.17    class_weight == 'balanced'"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPU cores used during the cross-validation loop. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any positive number for verbosity."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "refit",
        "init_args": {
          "semantic_types": [
            "refit"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "intercept_scaling",
        "init_args": {
          "semantic_types": [
            "intercept_scaling"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased."
        }
      },
      {
        "type": "Enumeration",
        "name": "multi_class",
        "init_args": {
          "semantic_types": [
            "multi_class"
          ],
          "values": [
            "auto",
            "ovr",
            "multinomial"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.  .. versionadded:: 0.18    Stochastic Average Gradient descent solver for 'multinomial' case. .. versionchanged:: 0.22     Default changed from 'ovr' to 'auto' in 0.22."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when `solver='sag'` or `solver='liblinear'`. Note that this only applies to the solver and not the cross-validation generator."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "l1_ratios",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratios"
          ],
          "default": "l1_ratios__None",
          "description": "The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to using ``penalty='l2'``, while 1 is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratios__list",
            "init_args": {
              "semantic_types": [
                "l1_ratios"
              ],
              "_structural_type": "list"
            }
          },
          {
            "type": "Constant",
            "name": "l1_ratios__None",
            "init_args": {
              "semantic_types": [
                "l1_ratios"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ]
  },
  "MultiTaskElasticNet": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskElasticNet",
    "common_name": "MultiTaskElasticNet",
    "description": "Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||_Fro^2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = sum_i sqrt(sum_j w_ij ^ 2)  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.5,
          "description": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ]
  },
  "MultiTaskElasticNetCV": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV",
    "common_name": "MultiTaskElasticNetCV",
    "description": "Multi-task L1/L2 ElasticNet with built-in cross-validation. See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.  .. versionadded:: 0.15",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "name": "l1_ratio",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "default": "l1_ratio__float",
          "description": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]``"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__float",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "float",
              "default": 0.5
            }
          },
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__list",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "list"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path"
        }
      },
      {
        "name": "alphas",
        "type": "Hyperparameter",
        "init_args": {
          "_structural_type": "ndarray",
          "semantic_types": [
            "alphas"
          ],
          "default": "None",
          "description": "List of alphas where to compute the models. If not provided, set automatically."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "Amount of verbosity."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. Note that this is used only if multiple values for l1_ratio are given. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ]
  },
  "MultiTaskLasso": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskLasso",
    "common_name": "MultiTaskLasso",
    "description": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4"
        }
      }
    ]
  },
  "MultiTaskLassoCV": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskLassoCV",
    "common_name": "MultiTaskLassoCV",
    "description": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskLasso is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  .. versionadded:: 0.15",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path"
        }
      },
      {
        "name": "alphas",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "default": "alphas__None",
          "description": "List of alphas where to compute the models. If not provided, set automatically."
        },
        "hyperparams": [
          {
            "name": "alphas__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alphas"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "alphas__None",
            "init_args": {
              "semantic_types": [
                "alphas"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Amount of verbosity."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. Note that this is used only if multiple values for l1_ratio are given. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``selection`` == 'random'"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ]
  },
  "OrthogonalMatchingPursuit": {
    "name": "sklearn.linear_model._omp.OrthogonalMatchingPursuit",
    "common_name": "OrthogonalMatchingPursuit",
    "description": "Orthogonal Matching Pursuit model (OMP) Read more in the :ref:`User Guide <omp>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "name": "n_nonzero_coefs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_nonzero_coefs"
          ],
          "default": "n_nonzero_coefs__None",
          "description": "Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_nonzero_coefs__int",
            "init_args": {
              "semantic_types": [
                "n_nonzero_coefs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_nonzero_coefs__None",
            "init_args": {
              "semantic_types": [
                "n_nonzero_coefs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "tol",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "default": "tol__None",
          "description": "Maximum norm of the residual. If not None, overrides n_nonzero_coefs."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "tol__float",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "tol__None",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when :term:`n_targets` or :term:`n_samples` is very large. Note that if you already have such matrices, you can pass them directly to the fit method."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          }
        ]
      }
    ]
  },
  "OrthogonalMatchingPursuitCV": {
    "name": "sklearn.linear_model._omp.OrthogonalMatchingPursuitCV",
    "common_name": "OrthogonalMatchingPursuitCV",
    "description": "Cross-validated Orthogonal Matching Pursuit model (OMP). See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <omp>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "copy",
        "init_args": {
          "semantic_types": [
            "copy"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "max_iter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "default": "max_iter__None",
          "description": "Maximum numbers of iterations to perform, therefore maximum features to include. 10% of ``n_features`` but at least 5 if available."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "max_iter__int",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "max_iter__None",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "n_jobs",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "_structural_type": "int",
          "default": "None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "Sets the verbosity amount"
        }
      }
    ]
  },
  "PassiveAggressiveClassifier": {
    "name": "sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier",
    "common_name": "PassiveAggressiveClassifier",
    "description": "Passive Aggressive Classifier Read more in the :ref:`User Guide <passive_aggressive>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "C",
        "init_args": {
          "semantic_types": [
            "C"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Maximum step size (regularization). Defaults to 1.0."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "name": "tol",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "default": "tol__float",
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).  .. versionadded:: 0.19"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "tol__float",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "float",
              "default": 0.001
            }
          },
          {
            "type": "Constant",
            "name": "tol__None",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "hinge",
          "description": "The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``  .. versionadded:: 0.17    parameter *class_weight* to automatically weight samples."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "average",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.  .. versionadded:: 0.19    parameter *average* to use weights averaging in SGD"
        }
      }
    ]
  },
  "PassiveAggressiveRegressor": {
    "name": "sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor",
    "common_name": "PassiveAggressiveRegressor",
    "description": "Passive Aggressive Regressor Read more in the :ref:`User Guide <passive_aggressive>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "C",
        "init_args": {
          "semantic_types": [
            "C"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Maximum step size (regularization). Defaults to 1.0."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "epsilon_insensitive",
          "description": "The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "If the difference between the current prediction and the correct label is below this threshold, the model is not updated."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "random_state",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "_structural_type": "int",
          "default": "None",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "average",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.  .. versionadded:: 0.19    parameter *average* to use weights averaging in SGD"
        }
      }
    ]
  },
  "Perceptron": {
    "name": "sklearn.linear_model._perceptron.Perceptron",
    "common_name": "Perceptron",
    "description": "Perceptron Read more in the :ref:`User Guide <perceptron>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "name": "penalty",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "default": "penalty__None",
          "description": "The penalty (aka regularization term) to be used."
        },
        "hyperparams": [
          {
            "type": "Enumeration",
            "name": "penalty__str",
            "init_args": {
              "semantic_types": [
                "penalty"
              ],
              "values": [
                "l2",
                "l1",
                "elasticnet"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "penalty__None",
            "init_args": {
              "semantic_types": [
                "penalty"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Constant that multiplies the regularization term if regularization is used."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eta0",
        "init_args": {
          "semantic_types": [
            "eta0"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant by which the updates are multiplied."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__int",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int",
              "default": 0
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      }
    ]
  },
  "RANSACRegressor": {
    "name": "sklearn.linear_model._ransac.RANSACRegressor",
    "common_name": "RANSACRegressor",
    "description": "RANSAC (RANdom SAmple Consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set.  Read more in the :ref:`User Guide <ransac_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "name": "base_estimator",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "base_estimator"
          ],
          "default": "base_estimator__None",
          "description": "Base estimator object which implements the following methods:   * `fit(X, y)`: Fit model to given training data and target values.  * `score(X, y)`: Returns the mean accuracy on the given test data,    which is used for the stop criterion defined by `stop_score`.    Additionally, the score is used to decide which of two equally    large consensus sets is chosen as the better one.  * `predict(X)`: Returns predicted values using the linear model,    which is used to compute residual error using loss function.  If `base_estimator` is None, then ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for target values of dtype float.  Note that the current implementation only supports regression estimators."
        },
        "hyperparams": [
          {
            "name": "base_estimator__Estimator",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "Estimator",
              "semantic_types": [
                "base_estimator"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "base_estimator__None",
            "init_args": {
              "semantic_types": [
                "base_estimator"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "min_samples",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "min_samples"
          ],
          "default": "min_samples__None",
          "description": "Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for `min_samples >= 1`, treated as a relative number `ceil(min_samples * X.shape[0]`) for `min_samples < 1`. This is typically chosen as the minimal number of samples necessary to estimate the given `base_estimator`. By default a ``sklearn.linear_model.LinearRegression()`` estimator is assumed and `min_samples` is chosen as ``X.shape[1] + 1``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "min_samples__int",
            "init_args": {
              "semantic_types": [
                "min_samples"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "min_samples__float",
            "init_args": {
              "semantic_types": [
                "min_samples"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "min_samples__None",
            "init_args": {
              "semantic_types": [
                "min_samples"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "residual_threshold",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "residual_threshold"
          ],
          "default": "residual_threshold__None",
          "description": "Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values `y`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "residual_threshold__float",
            "init_args": {
              "semantic_types": [
                "residual_threshold"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "residual_threshold__None",
            "init_args": {
              "semantic_types": [
                "residual_threshold"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "is_data_valid",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "is_data_valid"
          ],
          "default": "is_data_valid__None",
          "description": "This function is called with the randomly selected data before the model is fitted to it: `is_data_valid(X, y)`. If its return value is False the current randomly chosen sub-sample is skipped."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "is_data_valid__Callable",
            "init_args": {
              "semantic_types": [
                "is_data_valid"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "is_data_valid__None",
            "init_args": {
              "semantic_types": [
                "is_data_valid"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "is_model_valid",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "is_model_valid"
          ],
          "default": "is_model_valid__None",
          "description": "This function is called with the estimated model and the randomly selected data: `is_model_valid(model, X, y)`. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with `is_data_valid`. `is_model_valid` should therefore only be used if the estimated model is needed for making the rejection decision."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "is_model_valid__Callable",
            "init_args": {
              "semantic_types": [
                "is_model_valid"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "is_model_valid__None",
            "init_args": {
              "semantic_types": [
                "is_model_valid"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_trials",
        "init_args": {
          "semantic_types": [
            "max_trials"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations for random sample selection."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_skips",
        "init_args": {
          "semantic_types": [
            "max_skips"
          ],
          "_structural_type": "float",
          "default": Infinity,
          "description": "Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by ``is_data_valid`` or invalid models defined by ``is_model_valid``.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "stop_n_inliers",
        "init_args": {
          "semantic_types": [
            "stop_n_inliers"
          ],
          "_structural_type": "float",
          "default": Infinity,
          "description": "Stop iteration if at least this number of inliers are found."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "stop_score",
        "init_args": {
          "semantic_types": [
            "stop_score"
          ],
          "_structural_type": "float",
          "default": Infinity,
          "description": "Stop iteration if score is greater equal than this threshold."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "stop_probability",
        "init_args": {
          "semantic_types": [
            "stop_probability"
          ],
          "_structural_type": "float",
          "default": 0.99,
          "description": "RANSAC iteration stops if at least one outlier-free set of the training data is sampled in RANSAC. This requires to generate at least N samples (iterations)::      N >= log(1 - probability) / log(1 - e**m)  where the probability (confidence) is typically set to high value such as 0.99 (the default) and e is the current fraction of inliers w.r.t. the total number of samples."
        }
      },
      {
        "name": "loss",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "default": "loss__str",
          "description": "String inputs, \"absolute_loss\" and \"squared_loss\" are supported which find the absolute loss and squared loss per sample respectively.  If ``loss`` is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on ``X[i]``.  If the loss on a sample is greater than the ``residual_threshold``, then this sample is classified as an outlier."
        },
        "hyperparams": [
          {
            "type": "Enumeration",
            "name": "loss__str",
            "init_args": {
              "semantic_types": [
                "loss"
              ],
              "values": [
                "absolute_loss",
                "squared_loss"
              ],
              "_structural_type": "str",
              "default": "absolute_loss"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "loss__Callable",
            "init_args": {
              "semantic_types": [
                "loss"
              ],
              "_structural_type": "Callable"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The generator used to initialize the centers.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ]
  },
  "Ridge": {
    "name": "sklearn.linear_model._ridge.Ridge",
    "common_name": "Ridge",
    "description": "Linear least squares with l2 regularization. Minimizes the objective function::  ||y - Xw||^2_2 + alpha * ||w||^2_2  This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "name": "alpha",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "default": "alpha__float",
          "description": "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "alpha__float",
            "init_args": {
              "semantic_types": [
                "alpha"
              ],
              "_structural_type": "float",
              "default": 1.0
            }
          },
          {
            "name": "alpha__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alpha"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "max_iter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "default": "max_iter__None",
          "description": "Maximum number of iterations for conjugate gradient solver. For 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' solver, the default value is 1000."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "max_iter__int",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "max_iter__None",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Precision of the solution."
        }
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "auto",
            "svd",
            "cholesky",
            "lsqr",
            "sparse_cg",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge   coefficients. More stable for singular matrices than   'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to   obtain a closed-form solution.  - 'sparse_cg' uses the conjugate gradient solver as found in   scipy.sparse.linalg.cg. As an iterative algorithm, this solver is   more appropriate than 'cholesky' for large-scale data   (possibility to set `tol` and `max_iter`).  - 'lsqr' uses the dedicated regularized least-squares routine   scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative   procedure.  - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses   its improved, unbiased version named SAGA. Both methods also use an   iterative procedure, and are often faster than other solvers when   both n_samples and n_features are large. Note that 'sag' and   'saga' fast convergence is only guaranteed on features with   approximately the same scale. You can preprocess the data with a   scaler from sklearn.preprocessing.  All last five solvers support both dense and sparse data. However, only 'sparse_cg' supports sparse input when `fit_intercept` is True.  .. versionadded:: 0.17    Stochastic Average Gradient descent solver. .. versionadded:: 0.19    SAGA solver."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``solver`` == 'sag'.  .. versionadded:: 0.17    *random_state* to support Stochastic Average Gradient."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ]
  },
  "RidgeCV": {
    "name": "sklearn.linear_model._ridge.RidgeCV",
    "common_name": "RidgeCV",
    "description": "Ridge regression with built-in cross-validation. See glossary entry for :term:`cross-validation estimator`.  By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alphas",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "_structural_type": "tuple",
          "default": "&esc(0.1, 1.0, 10.0)",
          "description": "Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. If using generalized cross-validation, alphas must be positive."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "scoring",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "scoring"
          ],
          "default": "scoring__None",
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If None, the negative mean squared error if cv is 'auto' or None (i.e. when using generalized cross-validation), and r2 score otherwise."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "scoring__str",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "scoring__Callable",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "scoring__None",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation   (also known as Generalized Cross-Validation). - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if ``y`` is binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used, else, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "gcv_mode",
        "init_args": {
          "semantic_types": [
            "gcv_mode"
          ],
          "values": [
            "auto",
            "svd",
            "eigen"
          ],
          "_structural_type": "str",
          "default": "None",
          "description": "Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are::      'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'     'svd' : force use of singular value decomposition of X when X is         dense, eigenvalue decomposition of X^T.X when X is sparse.     'eigen' : force computation via eigendecomposition of X.X^T  The 'auto' mode is the default and is intended to pick the cheaper option of the two depending on the shape of the training data."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "store_cv_values",
        "init_args": {
          "semantic_types": [
            "store_cv_values"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_values_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Generalized Cross-Validation)."
        }
      }
    ]
  },
  "RidgeClassifier": {
    "name": "sklearn.linear_model._ridge.RidgeClassifier",
    "common_name": "RidgeClassifier",
    "description": "Classifier using Ridge regression. This classifier first converts the target values into ``{-1, 1}`` and then treats the problem as a regression task (multi-output regression in the multiclass case).  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "max_iter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "default": "max_iter__None",
          "description": "Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "max_iter__int",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "max_iter__None",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Precision of the solution."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "auto",
            "svd",
            "cholesky",
            "lsqr",
            "sparse_cg",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge   coefficients. More stable for singular matrices than   'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to   obtain a closed-form solution.  - 'sparse_cg' uses the conjugate gradient solver as found in   scipy.sparse.linalg.cg. As an iterative algorithm, this solver is   more appropriate than 'cholesky' for large-scale data   (possibility to set `tol` and `max_iter`).  - 'lsqr' uses the dedicated regularized least-squares routine   scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative   procedure.  - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses   its unbiased and more flexible version named SAGA. Both methods   use an iterative procedure, and are often faster than other solvers   when both n_samples and n_features are large. Note that 'sag' and   'saga' fast convergence is only guaranteed on features with   approximately the same scale. You can preprocess the data with a   scaler from sklearn.preprocessing.    .. versionadded:: 0.17      Stochastic Average Gradient descent solver.   .. versionadded:: 0.19    SAGA solver."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``solver`` == 'sag'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ]
  },
  "RidgeClassifierCV": {
    "name": "sklearn.linear_model._ridge.RidgeClassifierCV",
    "common_name": "RidgeClassifierCV",
    "description": "Ridge classifier with built-in cross-validation. See glossary entry for :term:`cross-validation estimator`.  By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation. Currently, only the n_features > n_samples case is handled efficiently.  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alphas",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "_structural_type": "tuple",
          "default": "&esc(0.1, 1.0, 10.0)",
          "description": "Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "scoring",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "scoring"
          ],
          "default": "None",
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "scoring__str",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "scoring__Callable",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "Callable"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "store_cv_values",
        "init_args": {
          "semantic_types": [
            "store_cv_values"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_values_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Generalized Cross-Validation)."
        }
      }
    ]
  },
  "SGDClassifier": {
    "name": "sklearn.linear_model._stochastic_gradient.SGDClassifier",
    "common_name": "SGDClassifier",
    "description": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.  This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  Read more in the :ref:`User Guide <sgd>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "hinge",
          "description": "The loss function to be used. Defaults to 'hinge', which gives a linear SVM.  The possible options are 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', or a regression loss: 'squared_loss', 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.  The 'log' loss gives logistic regression, a probabilistic classifier. 'modified_huber' is another smooth loss that brings tolerance to outliers as well as probability estimates. 'squared_hinge' is like hinge but is quadratically penalized. 'perceptron' is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description."
        }
      },
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l2",
            "l1",
            "elasticnet"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Constant that multiplies the regularization term. Defaults to 0.0001. Also used to compute learning_rate when set to 'optimal'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.15,
          "description": "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive epochs.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For 'huber', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "learning_rate",
        "init_args": {
          "semantic_types": [
            "learning_rate"
          ],
          "values": [
            "constant",
            "optimal",
            "invscaling",
            "adaptive"
          ],
          "_structural_type": "str",
          "default": "optimal",
          "description": "The learning rate schedule:  'constant':     eta = eta0 'optimal': [default]     eta = 1.0 / (alpha * (t + t0))     where t0 is chosen by a heuristic proposed by Leon Bottou. 'invscaling':     eta = eta0 / pow(t, power_t) 'adaptive':     eta = eta0, as long as the training keeps decreasing.     Each time n_iter_no_change consecutive epochs fail to decrease the     training loss by tol or fail to increase validation score by tol if     early_stopping is True, the current learning rate is divided by 5."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eta0",
        "init_args": {
          "semantic_types": [
            "eta0"
          ],
          "_structural_type": "float",
          "default": 0.0,
          "description": "The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules. The default value is 0.0 as eta0 is not used by the default schedule 'optimal'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "power_t",
        "init_args": {
          "semantic_types": [
            "power_t"
          ],
          "_structural_type": "float",
          "default": 0.5,
          "description": "The exponent for inverse scaling learning rate [default 0.5]."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling ``fit`` resets this counter, while ``partial_fit`` will result in increasing the existing counter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "average",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So ``average=10`` will begin averaging after seeing 10 samples."
        }
      }
    ]
  },
  "SGDRegressor": {
    "name": "sklearn.linear_model._stochastic_gradient.SGDRegressor",
    "common_name": "SGDRegressor",
    "description": "Linear model fitted by minimizing a regularized empirical loss with SGD SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  This implementation works with data represented as dense numpy arrays of floating point values for the features.  Read more in the :ref:`User Guide <sgd>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "squared_loss",
          "description": "The loss function to be used. The possible values are 'squared_loss', 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'  The 'squared_loss' refers to the ordinary least squares fit. 'huber' modifies 'squared_loss' to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. 'squared_epsilon_insensitive' is the same but becomes squared loss past a tolerance of epsilon."
        }
      },
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l2",
            "l1",
            "elasticnet"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Constant that multiplies the regularization term. Also used to compute learning_rate when set to 'optimal'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.15,
          "description": "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive epochs.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For 'huber', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "learning_rate",
        "init_args": {
          "semantic_types": [
            "learning_rate"
          ],
          "values": [
            "constant",
            "optimal",
            "invscaling",
            "adaptive"
          ],
          "_structural_type": "str",
          "default": "invscaling",
          "description": "The learning rate schedule:  'constant':     eta = eta0 'optimal':     eta = 1.0 / (alpha * (t + t0))     where t0 is chosen by a heuristic proposed by Leon Bottou. 'invscaling': [default]     eta = eta0 / pow(t, power_t) 'adaptive':     eta = eta0, as long as the training keeps decreasing.     Each time n_iter_no_change consecutive epochs fail to decrease the     training loss by tol or fail to increase validation score by tol if     early_stopping is True, the current learning rate is divided by 5."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eta0",
        "init_args": {
          "semantic_types": [
            "eta0"
          ],
          "_structural_type": "float",
          "default": 0.01,
          "description": "The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules. The default value is 0.01."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "power_t",
        "init_args": {
          "semantic_types": [
            "power_t"
          ],
          "_structural_type": "float",
          "default": 0.25,
          "description": "The exponent for inverse scaling learning rate."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling ``fit`` resets this counter, while ``partial_fit``  will result in increasing the existing counter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "average",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "_structural_type": "int",
          "default": "False",
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So ``average=10`` will begin averaging after seeing 10 samples."
        }
      }
    ]
  },
  "TheilSenRegressor": {
    "name": "sklearn.linear_model._theil_sen.TheilSenRegressor",
    "common_name": "TheilSenRegressor",
    "description": "Theil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is \"n_samples choose n_subsamples\", it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.  Read more in the :ref:`User Guide <theil_sen_regression>`.",
    "sklearn_version": "0.22.2.post1",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_subpopulation",
        "init_args": {
          "semantic_types": [
            "max_subpopulation"
          ],
          "_structural_type": "int",
          "default": 10000.0,
          "description": "Instead of computing with a set of cardinality 'n choose k', where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if 'n choose k' is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed."
        }
      },
      {
        "name": "n_subsamples",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_subsamples"
          ],
          "default": "n_subsamples__None",
          "description": "Number of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_subsamples__int",
            "init_args": {
              "semantic_types": [
                "n_subsamples"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_subsamples__None",
            "init_args": {
              "semantic_types": [
                "n_subsamples"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 300,
          "description": "Maximum number of iterations for the calculation of spatial median."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Tolerance when calculating spatial median."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "A random number generator instance to define the state of the random permutations generator.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Verbose mode when fitting the model."
        }
      }
    ]
  }
}