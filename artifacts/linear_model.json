{
  "ARDRegression": {
    "name": "sklearn.linear_model._bayes.ARDRegression",
    "common_name": "ARDRegression",
    "description": "Bayesian ARD regression. Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)  Read more in the :ref:`User Guide <bayesian_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "n_iter",
        "init_args": {
          "semantic_types": [
            "n_iter"
          ],
          "_structural_type": "int",
          "default": 300,
          "description": "Maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Stop the algorithm if w has converged."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_1",
        "init_args": {
          "semantic_types": [
            "alpha_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_2",
        "init_args": {
          "semantic_types": [
            "alpha_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_1",
        "init_args": {
          "semantic_types": [
            "lambda_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_2",
        "init_args": {
          "semantic_types": [
            "lambda_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "compute_score",
        "init_args": {
          "semantic_types": [
            "compute_score"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If True, compute the objective function at each step of the model."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "threshold_lambda",
        "init_args": {
          "semantic_types": [
            "threshold_lambda"
          ],
          "_structural_type": "float",
          "default": 10000.0,
          "description": "threshold for removing (pruning) weights with high precision from the computation."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Verbose mode when fitting the model."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array-like of shape (n_features,)",
        "description": "Coefficients of the regression model (mean of distribution)"
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "estimated precision of the noise."
      },
      {
        "name": "lambda_",
        "type": "array-like of shape (n_features,)",
        "description": "estimated precisions of the weights."
      },
      {
        "name": "sigma_",
        "type": "array-like of shape (n_features, n_features)",
        "description": "estimated variance-covariance matrix of the weights"
      },
      {
        "name": "scores_",
        "type": "float",
        "description": "if computed, value of the objective function (to be maximized)"
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``."
      },
      {
        "name": "X_offset_",
        "type": "float",
        "description": "If `normalize=True`, offset subtracted for centering data to a zero mean."
      },
      {
        "name": "X_scale_",
        "type": "float",
        "description": "If `normalize=True`, parameter used to scale data to a unit standard deviation."
      }
    ]
  },
  "BayesianRidge": {
    "name": "sklearn.linear_model._bayes.BayesianRidge",
    "common_name": "BayesianRidge",
    "description": "Bayesian ridge regression. Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "n_iter",
        "init_args": {
          "semantic_types": [
            "n_iter"
          ],
          "_structural_type": "int",
          "default": 300,
          "description": "Maximum number of iterations. Should be greater than or equal to 1."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Stop the algorithm if w has converged."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_1",
        "init_args": {
          "semantic_types": [
            "alpha_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_2",
        "init_args": {
          "semantic_types": [
            "alpha_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_1",
        "init_args": {
          "semantic_types": [
            "lambda_1"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_2",
        "init_args": {
          "semantic_types": [
            "lambda_2"
          ],
          "_structural_type": "float",
          "default": 1e-06,
          "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_init",
        "init_args": {
          "semantic_types": [
            "alpha_init"
          ],
          "_structural_type": "float",
          "default": "None",
          "description": "Initial value for alpha (precision of the noise). If not set, alpha_init is 1/Var(y).      .. versionadded:: 0.22"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "lambda_init",
        "init_args": {
          "semantic_types": [
            "lambda_init"
          ],
          "_structural_type": "float",
          "default": "None",
          "description": "Initial value for lambda (precision of the weights). If not set, lambda_init is 1.      .. versionadded:: 0.22"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "compute_score",
        "init_args": {
          "semantic_types": [
            "compute_score"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If True, compute the log marginal likelihood at each iteration of the optimization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. The intercept is not treated as a probabilistic parameter and thus has no associated variance. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Verbose mode when fitting the model."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array-like of shape (n_features,)",
        "description": "Coefficients of the regression model (mean of distribution)"
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``."
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "Estimated precision of the noise."
      },
      {
        "name": "lambda_",
        "type": "float",
        "description": "Estimated precision of the weights."
      },
      {
        "name": "sigma_",
        "type": "array-like of shape (n_features, n_features)",
        "description": "Estimated variance-covariance matrix of the weights"
      },
      {
        "name": "scores_",
        "type": "array-like of shape (n_iter_+1,)",
        "description": "If computed_score is True, value of the log marginal likelihood (to be maximized) at each iteration of the optimization. The array starts with the value of the log marginal likelihood obtained for the initial values of alpha and lambda and ends with the value obtained for the estimated alpha and lambda."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "The actual number of iterations to reach the stopping criterion."
      },
      {
        "name": "X_offset_",
        "type": "float",
        "description": "If `normalize=True`, offset subtracted for centering data to a zero mean."
      },
      {
        "name": "X_scale_",
        "type": "float",
        "description": "If `normalize=True`, parameter used to scale data to a unit standard deviation."
      }
    ]
  },
  "ElasticNet": {
    "name": "sklearn.linear_model._coordinate_descent.ElasticNet",
    "common_name": "ElasticNet",
    "description": "Linear regression with combined L1 and L2 priors as regularizer. Minimizes the objective function::          1 / (2 * n_samples) * ||y - Xw||^2_2         + alpha * l1_ratio * ||w||_1         + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to::          a * L1 + b * L2  where::          alpha = a + b and l1_ratio = a / (a + b)  The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.  Read more in the :ref:`User Guide <elastic_net>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.5,
          "description": "The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If ``False``, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__bool",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, forces the coefficients to be positive."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the cost function formula)."
      },
      {
        "name": "sparse_coef_",
        "type": "sparse matrix of shape (n_features,) or             (n_tasks, n_features)",
        "description": "Sparse representation of the `coef_`."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "n_iter_",
        "type": "list of int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance."
      },
      {
        "name": "dual_gap_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y."
      }
    ]
  },
  "ElasticNetCV": {
    "name": "sklearn.linear_model._coordinate_descent.ElasticNetCV",
    "common_name": "ElasticNetCV",
    "description": "Elastic Net model with iterative fitting along a regularization path. See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <elastic_net>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "l1_ratio",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "default": "l1_ratio__float",
          "description": "float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For ``l1_ratio = 0`` the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__float",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "float",
              "default": 0.5
            }
          },
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__list",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "list"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path, used for each l1_ratio."
        }
      },
      {
        "name": "alphas",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "default": "alphas__None",
          "description": "List of alphas where to compute the models. If None alphas are set automatically."
        },
        "hyperparams": [
          {
            "name": "alphas__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alphas"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "alphas__None",
            "init_args": {
              "semantic_types": [
                "alphas"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For int/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__int",
          "description": "Amount of verbosity."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int",
              "default": 0
            }
          }
        ]
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, forces the coefficients to be positive."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ],
    "Params": [
      {
        "name": "alpha_",
        "type": "float",
        "description": "The amount of penalization chosen by cross validation."
      },
      {
        "name": "l1_ratio_",
        "type": "float",
        "description": "The compromise between l1 and l2 penalization chosen by cross validation."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the cost function formula)."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets, n_features)",
        "description": "Independent term in the decision function."
      },
      {
        "name": "mse_path_",
        "type": "ndarray of shape (n_l1_ratio, n_alpha, n_folds)",
        "description": "Mean square error for the test set on each fold, varying l1_ratio and alpha."
      },
      {
        "name": "alphas_",
        "type": "ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)",
        "description": "The grid of alphas used for fitting, for each l1_ratio."
      },
      {
        "name": "dual_gap_",
        "type": "float",
        "description": "The dual gaps at the end of the optimization for the optimal alpha."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."
      }
    ]
  },
  "GammaRegressor": {
    "name": "sklearn.linear_model._glm.glm.GammaRegressor",
    "common_name": "GammaRegressor",
    "description": "Generalized Linear Model with a Gamma distribution. Read more in the :ref:`User Guide <Generalized_linear_regression>`.  .. versionadded:: 0.23",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the penalty term and thus determines the regularization strength. ``alpha = 0`` is equivalent to unpenalized GLMs. In this case, the design matrix `X` must have full column rank (no collinearities)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (X @ coef + intercept)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "The maximal number of iterations for the solver."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Stopping criterion. For the lbfgs solver, the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol`` where ``g_j`` is the j-th component of the gradient (derivative) of the objective function."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If set to ``True``, reuse the solution of the previous call to ``fit`` as initialization for ``coef_`` and ``intercept_`` ."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "For the lbfgs solver set verbose to any positive number for verbosity."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array of shape (n_features,)",
        "description": "Estimated coefficients for the linear predictor (`X * coef_ + intercept_`) in the GLM."
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "Intercept (a.k.a. bias) added to linear predictor."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Actual number of iterations used in the solver."
      }
    ]
  },
  "HuberRegressor": {
    "name": "sklearn.linear_model._huber.HuberRegressor",
    "common_name": "HuberRegressor",
    "description": "Linear regression model that is robust to outliers. The Huber Regressor optimizes the squared loss for the samples where ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.  This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.  Read more in the :ref:`User Guide <huber_regression>`  .. versionadded:: 0.18",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 1.35,
          "description": "The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations that ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Regularization parameter."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not to fit the intercept. This can be set to False if the data is already centered around the origin."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 1e-05,
          "description": "The iteration will stop when ``max{|proj g_i | i = 1, ..., n}`` <= ``tol`` where pg_i is the i-th component of the projected gradient."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array, shape (n_features,)",
        "description": "Features got by optimizing the Huber loss."
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "Bias."
      },
      {
        "name": "scale_",
        "type": "float",
        "description": "The value by which ``|y - X'w - c|`` is scaled down."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations that ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.  .. versionchanged:: 0.20      In SciPy <= 1.0.0 the number of lbfgs iterations may exceed     ``max_iter``. ``n_iter_`` will now report at most ``max_iter``."
      },
      {
        "name": "outliers_",
        "type": "array, shape (n_samples,)",
        "description": "A boolean mask which is set to True where the samples are identified as outliers."
      }
    ]
  },
  "Lars": {
    "name": "sklearn.linear_model._least_angle.Lars",
    "common_name": "Lars",
    "description": "Least Angle Regression model a.k.a. LAR Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__bool",
          "description": "Sets the verbosity amount."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "n_nonzero_coefs",
        "init_args": {
          "semantic_types": [
            "n_nonzero_coefs"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Target number of non-zero coefficients. Use ``np.inf`` for no limit."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_path",
        "init_args": {
          "semantic_types": [
            "fit_path"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha."
        }
      },
      {
        "name": "jitter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "jitter"
          ],
          "default": "jitter__None",
          "description": "Upper bound on a uniform noise parameter to be added to the `y` values, to satisfy the model's assumption of one-at-a-time computations. Might help with stability.  .. versionadded:: 0.23"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "jitter__int",
            "init_args": {
              "semantic_types": [
                "jitter"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "jitter__None",
            "init_args": {
              "semantic_types": [
                "jitter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Determines random number generation for jittering. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`. Ignored if `jitter` is None.  .. versionadded:: 0.23"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "alphas_",
        "type": "array-like of shape (n_alphas + 1,) or list of thereof of             shape (n_targets,)",
        "description": "Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller."
      },
      {
        "name": "active_",
        "type": "list of shape (n_alphas,) or list of thereof of shape             (n_targets,)",
        "description": "Indices of active variables at the end of the path."
      },
      {
        "name": "coef_path_",
        "type": "array-like of shape (n_features, n_alphas + 1) or list of             thereof of shape (n_targets,)",
        "description": "The varying values of the coefficients along the path. It is not present if the ``fit_path`` parameter is ``False``."
      },
      {
        "name": "coef_",
        "type": "array-like of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the formulation formula)."
      },
      {
        "name": "intercept_",
        "type": "float or array-like of shape (n_targets,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "n_iter_",
        "type": "array-like or int",
        "description": "The number of iterations taken by lars_path to find the grid of alphas for each target."
      }
    ]
  },
  "LarsCV": {
    "name": "sklearn.linear_model._least_angle.LarsCV",
    "common_name": "LarsCV",
    "description": "Cross-validated Least Angle Regression model. See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Sets the verbosity amount."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_n_alphas",
        "init_args": {
          "semantic_types": [
            "max_n_alphas"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of points on the path used to compute the residuals in the cross-validation"
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      }
    ],
    "Params": [
      {
        "name": "active_",
        "type": "list of length n_alphas or list of thereof of shape (n_targets,)",
        "description": "Indices of active variables at the end of the path."
      },
      {
        "name": "coef_",
        "type": "array-like of shape (n_features,)",
        "description": "parameter vector (w in the formulation formula)"
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "independent term in decision function"
      },
      {
        "name": "coef_path_",
        "type": "array-like of shape (n_features, n_alphas)",
        "description": "the varying values of the coefficients along the path"
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "the estimated regularization parameter alpha"
      },
      {
        "name": "alphas_",
        "type": "array-like of shape (n_alphas,)",
        "description": "the different values of alpha along the path"
      },
      {
        "name": "cv_alphas_",
        "type": "array-like of shape (n_cv_alphas,)",
        "description": "all the values of alpha along the path for the different folds"
      },
      {
        "name": "mse_path_",
        "type": "array-like of shape (n_folds, n_cv_alphas)",
        "description": "the mean square error on left-out for each fold along the path (alpha values given by ``cv_alphas``)"
      },
      {
        "name": "n_iter_",
        "type": "array-like or int",
        "description": "the number of iterations run by Lars with the optimal alpha."
      }
    ]
  },
  "Lasso": {
    "name": "sklearn.linear_model._coordinate_descent.Lasso",
    "common_name": "Lasso",
    "description": "Linear Model trained with L1 prior as regularizer (aka the Lasso) The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Technically the Lasso model is optimizing the same objective function as the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).  Read more in the :ref:`User Guide <lasso>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the L1 term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__bool",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, forces the coefficients to be positive."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the cost function formula)."
      },
      {
        "name": "dual_gap_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y."
      },
      {
        "name": "sparse_coef_",
        "type": "sparse matrix of shape (n_features, 1) or             (n_targets, n_features)",
        "description": "Readonly property derived from ``coef_``."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "n_iter_",
        "type": "int or list of int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance."
      }
    ]
  },
  "LassoCV": {
    "name": "sklearn.linear_model._coordinate_descent.LassoCV",
    "common_name": "LassoCV",
    "description": "Lasso linear model with iterative fitting along a regularization path. See glossary entry for :term:`cross-validation estimator`.  The best model is selected by cross-validation.  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <lasso>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path."
        }
      },
      {
        "name": "alphas",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "default": "alphas__None",
          "description": "List of alphas where to compute the models. If ``None`` alphas are set automatically."
        },
        "hyperparams": [
          {
            "name": "alphas__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alphas"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "alphas__None",
            "init_args": {
              "semantic_types": [
                "alphas"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For int/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__bool",
          "description": "Amount of verbosity."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "n_jobs",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "_structural_type": "int",
          "default": "None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If positive, restrict regression coefficients to be positive."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ],
    "Params": [
      {
        "name": "alpha_",
        "type": "float",
        "description": "The amount of penalization chosen by cross validation."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the cost function formula)."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "mse_path_",
        "type": "ndarray of shape (n_alphas, n_folds)",
        "description": "Mean square error for the test set on each fold, varying alpha."
      },
      {
        "name": "alphas_",
        "type": "ndarray of shape (n_alphas,)",
        "description": "The grid of alphas used for fitting."
      },
      {
        "name": "dual_gap_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "The dual gap at the end of the optimization for the optimal alpha (``alpha_``)."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."
      }
    ]
  },
  "LassoLars": {
    "name": "sklearn.linear_model._least_angle.LassoLars",
    "common_name": "LassoLars",
    "description": "Lasso model fit with Least Angle Regression a.k.a. Lars It is a Linear Model trained with an L1 prior as regularizer.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the penalty term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by :class:`LinearRegression`. For numerical reasons, using ``alpha = 0`` with the LassoLars object is not advised and you should prefer the LinearRegression object."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__bool",
          "description": "Sets the verbosity amount."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_path",
        "init_args": {
          "semantic_types": [
            "fit_path"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True`` the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator."
        }
      },
      {
        "name": "jitter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "jitter"
          ],
          "default": "jitter__None",
          "description": "Upper bound on a uniform noise parameter to be added to the `y` values, to satisfy the model's assumption of one-at-a-time computations. Might help with stability.  .. versionadded:: 0.23"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "jitter__float",
            "init_args": {
              "semantic_types": [
                "jitter"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "jitter__None",
            "init_args": {
              "semantic_types": [
                "jitter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Determines random number generation for jittering. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`. Ignored if `jitter` is None.  .. versionadded:: 0.23"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "alphas_",
        "type": "array-like of shape (n_alphas + 1,) or list of thereof of shape             (n_targets,)",
        "description": "Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller."
      },
      {
        "name": "active_",
        "type": "list of length n_alphas or list of thereof of shape (n_targets,)",
        "description": "Indices of active variables at the end of the path."
      },
      {
        "name": "coef_path_",
        "type": "array-like of shape (n_features, n_alphas + 1) or list of             thereof of shape (n_targets,)",
        "description": "If a list is passed it's expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the ``fit_path`` parameter is ``False``."
      },
      {
        "name": "coef_",
        "type": "array-like of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the formulation formula)."
      },
      {
        "name": "intercept_",
        "type": "float or array-like of shape (n_targets,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "n_iter_",
        "type": "array-like or int",
        "description": "The number of iterations taken by lars_path to find the grid of alphas for each target."
      }
    ]
  },
  "LassoLarsCV": {
    "name": "sklearn.linear_model._least_angle.LassoLarsCV",
    "common_name": "LassoLarsCV",
    "description": "Cross-validated Lasso, using the LARS algorithm. See glossary entry for :term:`cross-validation estimator`.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__bool",
          "description": "Sets the verbosity amount."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_n_alphas",
        "init_args": {
          "semantic_types": [
            "max_n_alphas"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of points on the path used to compute the residuals in the cross-validation"
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsCV only makes sense for problems where a sparse solution is expected and/or reached."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array-like of shape (n_features,)",
        "description": "parameter vector (w in the formulation formula)"
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "independent term in decision function."
      },
      {
        "name": "coef_path_",
        "type": "array-like of shape (n_features, n_alphas)",
        "description": "the varying values of the coefficients along the path"
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "the estimated regularization parameter alpha"
      },
      {
        "name": "alphas_",
        "type": "array-like of shape (n_alphas,)",
        "description": "the different values of alpha along the path"
      },
      {
        "name": "cv_alphas_",
        "type": "array-like of shape (n_cv_alphas,)",
        "description": "all the values of alpha along the path for the different folds"
      },
      {
        "name": "mse_path_",
        "type": "array-like of shape (n_folds, n_cv_alphas)",
        "description": "the mean square error on left-out for each fold along the path (alpha values given by ``cv_alphas``)"
      },
      {
        "name": "n_iter_",
        "type": "array-like or int",
        "description": "the number of iterations run by Lars with the optimal alpha."
      },
      {
        "name": "active_",
        "type": "list of int",
        "description": "Indices of active variables at the end of the path."
      }
    ]
  },
  "LassoLarsIC": {
    "name": "sklearn.linear_model._least_angle.LassoLarsIC",
    "common_name": "LassoLarsIC",
    "description": "Lasso model fit with Lars using BIC or AIC for model selection The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.  Read more in the :ref:`User Guide <least_angle_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Enumeration",
        "name": "criterion",
        "init_args": {
          "semantic_types": [
            "criterion"
          ],
          "values": [
            "bic",
            "aic"
          ],
          "_structural_type": "str",
          "default": "aic",
          "description": "The type of criterion to use."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__bool",
          "description": "Sets the verbosity amount."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "name": "precompute__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "precompute"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 500,
          "description": "Maximum number of iterations to perform. Can be used for early stopping."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 2.220446049250313e-16,
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsIC only makes sense for problems where a sparse solution is expected and/or reached."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array-like of shape (n_features,)",
        "description": "parameter vector (w in the formulation formula)"
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "independent term in decision function."
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "the alpha parameter chosen by the information criterion"
      },
      {
        "name": "alphas_",
        "type": "array-like of shape (n_alphas + 1,) or list thereof",
        "description": "Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller. If a list, it will be of length `n_targets`."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "number of iterations run by lars_path to find the grid of alphas."
      },
      {
        "name": "criterion_",
        "type": "array-like of shape (n_alphas,)",
        "description": "The value of the information criteria ('aic', 'bic') across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of ``n_samples`` compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007)."
      }
    ]
  },
  "LinearRegression": {
    "name": "sklearn.linear_model._base.LinearRegression",
    "common_name": "LinearRegression",
    "description": "Ordinary least squares Linear Regression. LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "positive",
        "init_args": {
          "semantic_types": [
            "positive"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, forces the coefficients to be positive. This option is only supported for dense arrays.  .. versionadded:: 0.24"
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array of shape (n_features, ) or (n_targets, n_features)",
        "description": "Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features."
      },
      {
        "name": "rank_",
        "type": "int",
        "description": "Rank of matrix `X`. Only available when `X` is dense."
      },
      {
        "name": "singular_",
        "type": "array of shape (min(X, y),)",
        "description": "Singular values of `X`. Only available when `X` is dense."
      },
      {
        "name": "intercept_",
        "type": "float or array of shape (n_targets,)",
        "description": "Independent term in the linear model. Set to 0.0 if `fit_intercept = False`."
      }
    ]
  },
  "LogisticRegression": {
    "name": "sklearn.linear_model._logistic.LogisticRegression",
    "common_name": "LogisticRegression",
    "description": "Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr', and uses the cross-entropy loss if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs', 'sag', 'saga' and 'newton-cg' solvers.)  This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note that regularization is applied by default**. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver.  Read more in the :ref:`User Guide <logistic_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l1",
            "l2",
            "elasticnet",
            "none"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver. If 'none' (not supported by the liblinear solver), no regularization is applied.  .. versionadded:: 0.19    l1 penalty with SAGA solver (allowing 'multinomial' + L1)"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "dual",
        "init_args": {
          "semantic_types": [
            "dual"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Tolerance for stopping criteria."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "C",
        "init_args": {
          "semantic_types": [
            "C"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "intercept_scaling",
        "init_args": {
          "semantic_types": [
            "intercept_scaling"
          ],
          "_structural_type": "bool",
          "default": 1,
          "description": "Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  .. versionadded:: 0.17    *class_weight='balanced'*"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the data. See :term:`Glossary <random_state>` for details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "newton-cg",
            "lbfgs",
            "liblinear",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "lbfgs",
          "description": "Algorithm to use in the optimization problem.  - For small datasets, 'liblinear' is a good choice, whereas 'sag' and   'saga' are faster for large ones. - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'   handle multinomial loss; 'liblinear' is limited to one-versus-rest   schemes. - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty - 'liblinear' and 'saga' also handle L1 penalty - 'saga' also supports 'elasticnet' penalty - 'liblinear' does not support setting ``penalty='none'``  Note that 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  .. versionadded:: 0.17    Stochastic Average Gradient descent solver. .. versionadded:: 0.19    SAGA solver. .. versionchanged:: 0.22     The default solver changed from 'liblinear' to 'lbfgs' in 0.22."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations taken for the solvers to converge."
        }
      },
      {
        "type": "Enumeration",
        "name": "multi_class",
        "init_args": {
          "semantic_types": [
            "multi_class"
          ],
          "values": [
            "auto",
            "ovr",
            "multinomial"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.  .. versionadded:: 0.18    Stochastic Average Gradient descent solver for 'multinomial' case. .. versionchanged:: 0.22     Default changed from 'ovr' to 'auto' in 0.22."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See :term:`the Glossary <warm_start>`.  .. versionadded:: 0.17    *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPU cores used when parallelizing over classes if multi_class='ovr'\". This parameter is ignored when the ``solver`` is set to 'liblinear' regardless of whether 'multi_class' is specified or not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "l1_ratio",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "default": "l1_ratio__None",
          "description": "The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__float",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "l1_ratio__None",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "classes_",
        "type": "ndarray of shape (n_classes, )",
        "description": "A list of class labels known to the classifier."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (1, n_features) or (n_classes, n_features)",
        "description": "Coefficient of the features in the decision function.  `coef_` is of shape (1, n_features) when the given problem is binary. In particular, when `multi_class='multinomial'`, `coef_` corresponds to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False)."
      },
      {
        "name": "intercept_",
        "type": "ndarray of shape (1,) or (n_classes,)",
        "description": "Intercept (a.k.a. bias) added to the decision function.  If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape (1,) when the given problem is binary. In particular, when `multi_class='multinomial'`, `intercept_` corresponds to outcome 1 (True) and `-intercept_` corresponds to outcome 0 (False)."
      },
      {
        "name": "n_iter_",
        "type": "ndarray of shape (n_classes,) or (1, )",
        "description": "Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.  .. versionchanged:: 0.20      In SciPy <= 1.0.0 the number of lbfgs iterations may exceed     ``max_iter``. ``n_iter_`` will now report at most ``max_iter``."
      }
    ]
  },
  "LogisticRegressionCV": {
    "name": "sklearn.linear_model._logistic.LogisticRegressionCV",
    "common_name": "LogisticRegressionCV",
    "description": "Logistic Regression CV (aka logit, MaxEnt) classifier. See glossary entry for :term:`cross-validation estimator`.  This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. Elastic-Net penalty is only supported by the saga solver.  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter is selected by the cross-validator :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).  Read more in the :ref:`User Guide <logistic_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "Cs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "Cs"
          ],
          "default": "Cs__int",
          "description": "Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "Cs__int",
            "init_args": {
              "semantic_types": [
                "Cs"
              ],
              "_structural_type": "int",
              "default": 10
            }
          },
          {
            "type": "Hyperparameter",
            "name": "Cs__list",
            "init_args": {
              "semantic_types": [
                "Cs"
              ],
              "_structural_type": "list"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module :mod:`sklearn.model_selection` module for the list of possible cross-validation objects.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "dual",
        "init_args": {
          "semantic_types": [
            "dual"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features."
        }
      },
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l1",
            "l2",
            "elasticnet"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver."
        }
      },
      {
        "name": "scoring",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "scoring"
          ],
          "default": "scoring__None",
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. For a list of scoring functions that can be used, look at :mod:`sklearn.metrics`. The default scoring option used is 'accuracy'."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "scoring__str",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "scoring__Callable",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "scoring__None",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "newton-cg",
            "lbfgs",
            "liblinear",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "lbfgs",
          "description": "Algorithm to use in the optimization problem.  - For small datasets, 'liblinear' is a good choice, whereas 'sag' and   'saga' are faster for large ones. - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'   handle multinomial loss; 'liblinear' is limited to one-versus-rest   schemes. - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas   'liblinear' and 'saga' handle L1 penalty. - 'liblinear' might be slower in LogisticRegressionCV because it does   not handle warm-starting.  Note that 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  .. versionadded:: 0.17    Stochastic Average Gradient descent solver. .. versionadded:: 0.19    SAGA solver."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Tolerance for stopping criteria."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations of the optimization algorithm."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  .. versionadded:: 0.17    class_weight == 'balanced'"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPU cores used during the cross-validation loop. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any positive number for verbosity."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "refit",
        "init_args": {
          "semantic_types": [
            "refit"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "intercept_scaling",
        "init_args": {
          "semantic_types": [
            "intercept_scaling"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased."
        }
      },
      {
        "type": "Enumeration",
        "name": "multi_class",
        "init_args": {
          "semantic_types": [
            "multi_class"
          ],
          "values": [
            "auto",
            "ovr",
            "multinomial"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.  .. versionadded:: 0.18    Stochastic Average Gradient descent solver for 'multinomial' case. .. versionchanged:: 0.22     Default changed from 'ovr' to 'auto' in 0.22."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data. Note that this only applies to the solver and not the cross-validation generator. See :term:`Glossary <random_state>` for details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "l1_ratios",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratios"
          ],
          "default": "l1_ratios__None",
          "description": "The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to using ``penalty='l2'``, while 1 is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratios__list",
            "init_args": {
              "semantic_types": [
                "l1_ratios"
              ],
              "_structural_type": "list"
            }
          },
          {
            "type": "Constant",
            "name": "l1_ratios__None",
            "init_args": {
              "semantic_types": [
                "l1_ratios"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "classes_",
        "type": "ndarray of shape (n_classes, )",
        "description": "A list of class labels known to the classifier."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (1, n_features) or (n_classes, n_features)",
        "description": "Coefficient of the features in the decision function.  `coef_` is of shape (1, n_features) when the given problem is binary."
      },
      {
        "name": "intercept_",
        "type": "ndarray of shape (1,) or (n_classes,)",
        "description": "Intercept (a.k.a. bias) added to the decision function.  If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape(1,) when the problem is binary."
      },
      {
        "name": "Cs_",
        "type": "ndarray of shape (n_cs)",
        "description": "Array of C i.e. inverse of regularization parameter values used for cross-validation."
      },
      {
        "name": "l1_ratios_",
        "type": "ndarray of shape (n_l1_ratios)",
        "description": "Array of l1_ratios used for cross-validation. If no l1_ratio is used (i.e. penalty is not 'elasticnet'), this is set to ``[None]``"
      },
      {
        "name": "coefs_paths_",
        "type": "ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)",
        "description": "dict with classes as the keys, and the path of coefficients obtained during cross-validating across each fold and then across each Cs after doing an OvR for the corresponding class as values. If the 'multi_class' option is set to 'multinomial', then the coefs_paths are the coefficients corresponding to each class. Each dict value has shape ``(n_folds, n_cs, n_features)`` or ``(n_folds, n_cs, n_features + 1)`` depending on whether the intercept is fit or not. If ``penalty='elasticnet'``, the shape is ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``."
      },
      {
        "name": "scores_",
        "type": "dict",
        "description": "dict with classes as the keys, and the values as the grid of scores obtained during cross-validating each fold, after doing an OvR for the corresponding class. If the 'multi_class' option given is 'multinomial' then the same scores are repeated across all classes, since this is the multinomial class. Each dict value has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if ``penalty='elasticnet'``."
      },
      {
        "name": "C_",
        "type": "ndarray of shape (n_classes,) or (n_classes - 1,)",
        "description": "Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C's that correspond to the best scores for each fold. `C_` is of shape(n_classes,) when the problem is binary."
      },
      {
        "name": "l1_ratio_",
        "type": "ndarray of shape (n_classes,) or (n_classes - 1,)",
        "description": "Array of l1_ratio that maps to the best scores across every class. If refit is set to False, then for each class, the best l1_ratio is the average of the l1_ratio's that correspond to the best scores for each fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary."
      },
      {
        "name": "n_iter_",
        "type": "ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)",
        "description": "Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1. If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds, n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``."
      }
    ]
  },
  "MultiTaskElasticNet": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskElasticNet",
    "common_name": "MultiTaskElasticNet",
    "description": "Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer. The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||_Fro^2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)  i.e. the sum of norms of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.5,
          "description": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ],
    "Params": [
      {
        "name": "intercept_",
        "type": "ndarray of shape (n_tasks,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (n_tasks, n_features)",
        "description": "Parameter vector (W in the cost function formula). If a 1D y is passed in at fit (non multi-task usage), ``coef_`` is then a 1D array. Note that ``coef_`` stores the transpose of ``W``, ``W.T``."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance."
      },
      {
        "name": "dual_gap_",
        "type": "float",
        "description": "The dual gaps at the end of the optimization."
      },
      {
        "name": "eps_",
        "type": "float",
        "description": "The tolerance scaled scaled by the variance of the target `y`."
      },
      {
        "name": "sparse_coef_",
        "type": "sparse matrix of shape (n_features,) or             (n_tasks, n_features)",
        "description": "Sparse representation of the `coef_`."
      }
    ]
  },
  "MultiTaskElasticNetCV": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV",
    "common_name": "MultiTaskElasticNetCV",
    "description": "Multi-task L1/L2 ElasticNet with built-in cross-validation. See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.  .. versionadded:: 0.15",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "l1_ratio",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "default": "l1_ratio__float",
          "description": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]``"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__float",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "float",
              "default": 0.5
            }
          },
          {
            "type": "Hyperparameter",
            "name": "l1_ratio__list",
            "init_args": {
              "semantic_types": [
                "l1_ratio"
              ],
              "_structural_type": "list"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path."
        }
      },
      {
        "name": "alphas",
        "type": "Hyperparameter",
        "init_args": {
          "_structural_type": "ndarray",
          "semantic_types": [
            "alphas"
          ],
          "default": "None",
          "description": "List of alphas where to compute the models. If not provided, set automatically."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For int/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__int",
          "description": "Amount of verbosity."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int",
              "default": 0
            }
          }
        ]
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. Note that this is used only if multiple values for l1_ratio are given. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ],
    "Params": [
      {
        "name": "intercept_",
        "type": "ndarray of shape (n_tasks,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (n_tasks, n_features)",
        "description": "Parameter vector (W in the cost function formula). Note that ``coef_`` stores the transpose of ``W``, ``W.T``."
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "The amount of penalization chosen by cross validation."
      },
      {
        "name": "mse_path_",
        "type": "ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)",
        "description": "Mean square error for the test set on each fold, varying alpha."
      },
      {
        "name": "alphas_",
        "type": "ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)",
        "description": "The grid of alphas used for fitting, for each l1_ratio."
      },
      {
        "name": "l1_ratio_",
        "type": "float",
        "description": "Best l1_ratio obtained by cross-validation."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."
      },
      {
        "name": "dual_gap_",
        "type": "float",
        "description": "The dual gap at the end of the optimization for the optimal alpha."
      }
    ]
  },
  "MultiTaskLasso": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskLasso",
    "common_name": "MultiTaskLasso",
    "description": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4"
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (n_tasks, n_features)",
        "description": "Parameter vector (W in the cost function formula). Note that ``coef_`` stores the transpose of ``W``, ``W.T``."
      },
      {
        "name": "intercept_",
        "type": "ndarray of shape (n_tasks,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance."
      },
      {
        "name": "dual_gap_",
        "type": "ndarray of shape (n_alphas,)",
        "description": "The dual gaps at the end of the optimization for each alpha."
      },
      {
        "name": "eps_",
        "type": "float",
        "description": "The tolerance scaled scaled by the variance of the target `y`."
      },
      {
        "name": "sparse_coef_",
        "type": "sparse matrix of shape (n_features,) or             (n_tasks, n_features)",
        "description": "Sparse representation of the `coef_`."
      }
    ]
  },
  "MultiTaskLassoCV": {
    "name": "sklearn.linear_model._coordinate_descent.MultiTaskLassoCV",
    "common_name": "MultiTaskLassoCV",
    "description": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer. See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskLasso is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  .. versionadded:: 0.15",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "eps",
        "init_args": {
          "semantic_types": [
            "eps"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_alphas",
        "init_args": {
          "semantic_types": [
            "n_alphas"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Number of alphas along the regularization path."
        }
      },
      {
        "name": "alphas",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "default": "alphas__None",
          "description": "List of alphas where to compute the models. If not provided, set automatically."
        },
        "hyperparams": [
          {
            "name": "alphas__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alphas"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "alphas__None",
            "init_args": {
              "semantic_types": [
                "alphas"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of iterations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If ``True``, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For int/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Amount of verbosity."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. Note that this is used only if multiple values for l1_ratio are given. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "selection",
        "init_args": {
          "semantic_types": [
            "selection"
          ],
          "values": [
            "cyclic",
            "random"
          ],
          "_structural_type": "str",
          "default": "cyclic",
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4."
        }
      }
    ],
    "Params": [
      {
        "name": "intercept_",
        "type": "ndarray of shape (n_tasks,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (n_tasks, n_features)",
        "description": "Parameter vector (W in the cost function formula). Note that ``coef_`` stores the transpose of ``W``, ``W.T``."
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "The amount of penalization chosen by cross validation."
      },
      {
        "name": "mse_path_",
        "type": "ndarray of shape (n_alphas, n_folds)",
        "description": "Mean square error for the test set on each fold, varying alpha."
      },
      {
        "name": "alphas_",
        "type": "ndarray of shape (n_alphas,)",
        "description": "The grid of alphas used for fitting."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."
      },
      {
        "name": "dual_gap_",
        "type": "float",
        "description": "The dual gap at the end of the optimization for the optimal alpha."
      }
    ]
  },
  "OrthogonalMatchingPursuit": {
    "name": "sklearn.linear_model._omp.OrthogonalMatchingPursuit",
    "common_name": "OrthogonalMatchingPursuit",
    "description": "Orthogonal Matching Pursuit model (OMP). Read more in the :ref:`User Guide <omp>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "n_nonzero_coefs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_nonzero_coefs"
          ],
          "default": "n_nonzero_coefs__None",
          "description": "Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_nonzero_coefs__int",
            "init_args": {
              "semantic_types": [
                "n_nonzero_coefs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_nonzero_coefs__None",
            "init_args": {
              "semantic_types": [
                "n_nonzero_coefs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "tol",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "default": "tol__None",
          "description": "Maximum norm of the residual. If not None, overrides n_nonzero_coefs."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "tol__float",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "tol__None",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "precompute",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "precompute"
          ],
          "default": "precompute__str",
          "description": "Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when :term:`n_targets` or :term:`n_samples` is very large. Note that if you already have such matrices, you can pass them directly to the fit method."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "precompute__str",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "str",
              "default": "auto"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "precompute__bool",
            "init_args": {
              "semantic_types": [
                "precompute"
              ],
              "_structural_type": "bool"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the formula)."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "n_iter_",
        "type": "int or array-like",
        "description": "Number of active features across every target."
      },
      {
        "name": "n_nonzero_coefs_",
        "type": "int",
        "description": "The number of non-zero coefficients in the solution. If `n_nonzero_coefs` is None and `tol` is None this value is either set to 10% of `n_features` or 1, whichever is greater."
      }
    ]
  },
  "OrthogonalMatchingPursuitCV": {
    "name": "sklearn.linear_model._omp.OrthogonalMatchingPursuitCV",
    "common_name": "OrthogonalMatchingPursuitCV",
    "description": "Cross-validated Orthogonal Matching Pursuit model (OMP). See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <omp>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "copy",
        "init_args": {
          "semantic_types": [
            "copy"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "max_iter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "default": "max_iter__None",
          "description": "Maximum numbers of iterations to perform, therefore maximum features to include. 10% of ``n_features`` but at least 5 if available."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "max_iter__int",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "max_iter__None",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.22     ``cv`` default value if None changed from 3-fold to 5-fold."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "n_jobs",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "_structural_type": "int",
          "default": "None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        }
      },
      {
        "name": "verbose",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "default": "verbose__bool",
          "description": "Sets the verbosity amount."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "verbose__bool",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "verbose__int",
            "init_args": {
              "semantic_types": [
                "verbose"
              ],
              "_structural_type": "int"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
        "description": "Parameter vector (w in the problem formulation)."
      },
      {
        "name": "n_nonzero_coefs_",
        "type": "int",
        "description": "Estimated number of non-zero coefficients giving the best mean squared error over the cross-validation folds."
      },
      {
        "name": "n_iter_",
        "type": "int or array-like",
        "description": "Number of active features across every target for the model refit with the best hyperparameters got by cross-validating across all folds."
      }
    ]
  },
  "PassiveAggressiveClassifier": {
    "name": "sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier",
    "common_name": "PassiveAggressiveClassifier",
    "description": "Passive Aggressive Classifier Read more in the :ref:`User Guide <passive_aggressive>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "C",
        "init_args": {
          "semantic_types": [
            "C"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Maximum step size (regularization). Defaults to 1.0."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "name": "tol",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "default": "tol__float",
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).  .. versionadded:: 0.19"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "tol__float",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "float",
              "default": 0.001
            }
          },
          {
            "type": "Constant",
            "name": "tol__None",
            "init_args": {
              "semantic_types": [
                "tol"
              ],
              "_structural_type": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "hinge",
          "description": "The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Used to shuffle the training data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``  .. versionadded:: 0.17    parameter *class_weight* to automatically weight samples."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "average",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "default": "average__bool",
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.  .. versionadded:: 0.19    parameter *average* to use weights averaging in SGD"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "average__bool",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "average__int",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "int"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]",
        "description": "Weights assigned to the features."
      },
      {
        "name": "intercept_",
        "type": "array, shape = [1] if n_classes == 2 else [n_classes]",
        "description": "Constants in decision function."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit."
      },
      {
        "name": "classes_",
        "type": "array of shape (n_classes,)",
        "description": "The unique classes labels."
      },
      {
        "name": "t_",
        "type": "int",
        "description": "Number of weight updates performed during training. Same as ``(n_iter_ * n_samples)``."
      },
      {
        "name": "loss_function_",
        "type": "callable",
        "description": "Loss function used by the algorithm."
      }
    ]
  },
  "PassiveAggressiveRegressor": {
    "name": "sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor",
    "common_name": "PassiveAggressiveRegressor",
    "description": "Passive Aggressive Regressor Read more in the :ref:`User Guide <passive_aggressive>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "C",
        "init_args": {
          "semantic_types": [
            "C"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Maximum step size (regularization). Defaults to 1.0."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "epsilon_insensitive",
          "description": "The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "If the difference between the current prediction and the correct label is below this threshold, the model is not updated."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "random_state",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "_structural_type": "int",
          "default": "None",
          "description": "Used to shuffle the training data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled."
        }
      },
      {
        "name": "average",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "default": "average__bool",
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.  .. versionadded:: 0.19    parameter *average* to use weights averaging in SGD"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "average__bool",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "average__int",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "int"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]",
        "description": "Weights assigned to the features."
      },
      {
        "name": "intercept_",
        "type": "array, shape = [1] if n_classes == 2 else [n_classes]",
        "description": "Constants in decision function."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "The actual number of iterations to reach the stopping criterion."
      },
      {
        "name": "t_",
        "type": "int",
        "description": "Number of weight updates performed during training. Same as ``(n_iter_ * n_samples)``."
      }
    ]
  },
  "Perceptron": {
    "name": "sklearn.linear_model._perceptron.Perceptron",
    "common_name": "Perceptron",
    "description": "Perceptron Read more in the :ref:`User Guide <perceptron>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "penalty",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "default": "penalty__None",
          "description": "The penalty (aka regularization term) to be used."
        },
        "hyperparams": [
          {
            "type": "Enumeration",
            "name": "penalty__str",
            "init_args": {
              "semantic_types": [
                "penalty"
              ],
              "values": [
                "l2",
                "l1",
                "elasticnet"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "penalty__None",
            "init_args": {
              "semantic_types": [
                "penalty"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Constant that multiplies the regularization term if regularization is used."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.15,
          "description": "The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`. `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1. Only used if `penalty='elasticnet'`.  .. versionadded:: 0.24"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eta0",
        "init_args": {
          "semantic_types": [
            "eta0"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant by which the updates are multiplied."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__int",
          "description": "Used to shuffle the training data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int",
              "default": 0
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  .. versionadded:: 0.20"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20"
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`."
        }
      }
    ],
    "Params": [
      {
        "name": "classes_",
        "type": "ndarray of shape (n_classes,)",
        "description": "The unique classes labels."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)",
        "description": "Weights assigned to the features."
      },
      {
        "name": "intercept_",
        "type": "ndarray of shape (1,) if n_classes == 2 else (n_classes,)",
        "description": "Constants in decision function."
      },
      {
        "name": "loss_function_",
        "type": "concrete\u00a0LossFunction",
        "description": "The function that determines the loss, or difference between the output of the algorithm and the target values."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit."
      },
      {
        "name": "t_",
        "type": "int",
        "description": "Number of weight updates performed during training. Same as ``(n_iter_ * n_samples)``."
      }
    ]
  },
  "PoissonRegressor": {
    "name": "sklearn.linear_model._glm.glm.PoissonRegressor",
    "common_name": "PoissonRegressor",
    "description": "Generalized Linear Model with a Poisson distribution. Read more in the :ref:`User Guide <Generalized_linear_regression>`.  .. versionadded:: 0.23",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the penalty term and thus determines the regularization strength. ``alpha = 0`` is equivalent to unpenalized GLMs. In this case, the design matrix `X` must have full column rank (no collinearities)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (X @ coef + intercept)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "The maximal number of iterations for the solver."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Stopping criterion. For the lbfgs solver, the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol`` where ``g_j`` is the j-th component of the gradient (derivative) of the objective function."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If set to ``True``, reuse the solution of the previous call to ``fit`` as initialization for ``coef_`` and ``intercept_`` ."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "For the lbfgs solver set verbose to any positive number for verbosity."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array of shape (n_features,)",
        "description": "Estimated coefficients for the linear predictor (`X @ coef_ + intercept_`) in the GLM."
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "Intercept (a.k.a. bias) added to linear predictor."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Actual number of iterations used in the solver."
      }
    ]
  },
  "RANSACRegressor": {
    "name": "sklearn.linear_model._ransac.RANSACRegressor",
    "common_name": "RANSACRegressor",
    "description": "RANSAC (RANdom SAmple Consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set.  Read more in the :ref:`User Guide <ransac_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "base_estimator",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "base_estimator"
          ],
          "default": "base_estimator__None",
          "description": "Base estimator object which implements the following methods:   * `fit(X, y)`: Fit model to given training data and target values.  * `score(X, y)`: Returns the mean accuracy on the given test data,    which is used for the stop criterion defined by `stop_score`.    Additionally, the score is used to decide which of two equally    large consensus sets is chosen as the better one.  * `predict(X)`: Returns predicted values using the linear model,    which is used to compute residual error using loss function.  If `base_estimator` is None, then :class:`~sklearn.linear_model.LinearRegression` is used for target values of dtype float.  Note that the current implementation only supports regression estimators."
        },
        "hyperparams": [
          {
            "name": "base_estimator__Estimator",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "Estimator",
              "semantic_types": [
                "base_estimator"
              ]
            }
          },
          {
            "type": "Constant",
            "name": "base_estimator__None",
            "init_args": {
              "semantic_types": [
                "base_estimator"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "min_samples",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "min_samples"
          ],
          "default": "min_samples__None",
          "description": "Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for `min_samples >= 1`, treated as a relative number `ceil(min_samples * X.shape[0]`) for `min_samples < 1`. This is typically chosen as the minimal number of samples necessary to estimate the given `base_estimator`. By default a ``sklearn.linear_model.LinearRegression()`` estimator is assumed and `min_samples` is chosen as ``X.shape[1] + 1``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "min_samples__int",
            "init_args": {
              "semantic_types": [
                "min_samples"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "min_samples__float",
            "init_args": {
              "semantic_types": [
                "min_samples"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "min_samples__None",
            "init_args": {
              "semantic_types": [
                "min_samples"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "residual_threshold",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "residual_threshold"
          ],
          "default": "residual_threshold__None",
          "description": "Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values `y`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "residual_threshold__float",
            "init_args": {
              "semantic_types": [
                "residual_threshold"
              ],
              "_structural_type": "float"
            }
          },
          {
            "type": "Constant",
            "name": "residual_threshold__None",
            "init_args": {
              "semantic_types": [
                "residual_threshold"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "is_data_valid",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "is_data_valid"
          ],
          "default": "is_data_valid__None",
          "description": "This function is called with the randomly selected data before the model is fitted to it: `is_data_valid(X, y)`. If its return value is False the current randomly chosen sub-sample is skipped."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "is_data_valid__Callable",
            "init_args": {
              "semantic_types": [
                "is_data_valid"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "is_data_valid__None",
            "init_args": {
              "semantic_types": [
                "is_data_valid"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "is_model_valid",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "is_model_valid"
          ],
          "default": "is_model_valid__None",
          "description": "This function is called with the estimated model and the randomly selected data: `is_model_valid(model, X, y)`. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with `is_data_valid`. `is_model_valid` should therefore only be used if the estimated model is needed for making the rejection decision."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "is_model_valid__Callable",
            "init_args": {
              "semantic_types": [
                "is_model_valid"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "is_model_valid__None",
            "init_args": {
              "semantic_types": [
                "is_model_valid"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_trials",
        "init_args": {
          "semantic_types": [
            "max_trials"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "Maximum number of iterations for random sample selection."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_skips",
        "init_args": {
          "semantic_types": [
            "max_skips"
          ],
          "_structural_type": "float",
          "default": Infinity,
          "description": "Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by ``is_data_valid`` or invalid models defined by ``is_model_valid``.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "stop_n_inliers",
        "init_args": {
          "semantic_types": [
            "stop_n_inliers"
          ],
          "_structural_type": "float",
          "default": Infinity,
          "description": "Stop iteration if at least this number of inliers are found."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "stop_score",
        "init_args": {
          "semantic_types": [
            "stop_score"
          ],
          "_structural_type": "float",
          "default": Infinity,
          "description": "Stop iteration if score is greater equal than this threshold."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "stop_probability",
        "init_args": {
          "semantic_types": [
            "stop_probability"
          ],
          "_structural_type": "float",
          "default": 0.99,
          "description": "RANSAC iteration stops if at least one outlier-free set of the training data is sampled in RANSAC. This requires to generate at least N samples (iterations)::      N >= log(1 - probability) / log(1 - e**m)  where the probability (confidence) is typically set to high value such as 0.99 (the default) and e is the current fraction of inliers w.r.t. the total number of samples."
        }
      },
      {
        "name": "loss",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "default": "loss__str",
          "description": "String inputs, \"absolute_loss\" and \"squared_loss\" are supported which find the absolute loss and squared loss per sample respectively.  If ``loss`` is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on ``X[i]``.  If the loss on a sample is greater than the ``residual_threshold``, then this sample is classified as an outlier.  .. versionadded:: 0.18"
        },
        "hyperparams": [
          {
            "type": "Enumeration",
            "name": "loss__str",
            "init_args": {
              "semantic_types": [
                "loss"
              ],
              "values": [
                "absolute_loss",
                "squared_loss"
              ],
              "_structural_type": "str",
              "default": "absolute_loss"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "loss__Callable",
            "init_args": {
              "semantic_types": [
                "loss"
              ],
              "_structural_type": "Callable"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "The generator used to initialize the centers. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "estimator_",
        "type": "object",
        "description": "Best fitted model (copy of the `base_estimator` object)."
      },
      {
        "name": "n_trials_",
        "type": "int",
        "description": "Number of random selection trials until one of the stop criteria is met. It is always ``<= max_trials``."
      },
      {
        "name": "inlier_mask_",
        "type": "bool array of shape [n_samples]",
        "description": "Boolean mask of inliers classified as ``True``."
      },
      {
        "name": "n_skips_no_inliers_",
        "type": "int",
        "description": "Number of iterations skipped due to finding zero inliers.  .. versionadded:: 0.19"
      },
      {
        "name": "n_skips_invalid_data_",
        "type": "int",
        "description": "Number of iterations skipped due to invalid data defined by ``is_data_valid``.  .. versionadded:: 0.19"
      },
      {
        "name": "n_skips_invalid_model_",
        "type": "int",
        "description": "Number of iterations skipped due to an invalid model defined by ``is_model_valid``.  .. versionadded:: 0.19"
      }
    ]
  },
  "Ridge": {
    "name": "sklearn.linear_model._ridge.Ridge",
    "common_name": "Ridge",
    "description": "Linear least squares with l2 regularization. Minimizes the objective function::  ||y - Xw||^2_2 + alpha * ||w||^2_2  This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "alpha",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "default": "alpha__float",
          "description": "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "alpha__float",
            "init_args": {
              "semantic_types": [
                "alpha"
              ],
              "_structural_type": "float",
              "default": 1.0
            }
          },
          {
            "name": "alpha__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alpha"
              ]
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. ``X`` and ``y`` are expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "max_iter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "default": "max_iter__None",
          "description": "Maximum number of iterations for conjugate gradient solver. For 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' solver, the default value is 1000."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "max_iter__int",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "max_iter__None",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Precision of the solution."
        }
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "auto",
            "svd",
            "cholesky",
            "lsqr",
            "sparse_cg",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge   coefficients. More stable for singular matrices than 'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to   obtain a closed-form solution.  - 'sparse_cg' uses the conjugate gradient solver as found in   scipy.sparse.linalg.cg. As an iterative algorithm, this solver is   more appropriate than 'cholesky' for large-scale data   (possibility to set `tol` and `max_iter`).  - 'lsqr' uses the dedicated regularized least-squares routine   scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative   procedure.  - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses   its improved, unbiased version named SAGA. Both methods also use an   iterative procedure, and are often faster than other solvers when   both n_samples and n_features are large. Note that 'sag' and   'saga' fast convergence is only guaranteed on features with   approximately the same scale. You can preprocess the data with a   scaler from sklearn.preprocessing.  All last five solvers support both dense and sparse data. However, only 'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is True.  .. versionadded:: 0.17    Stochastic Average Gradient descent solver. .. versionadded:: 0.19    SAGA solver."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Used when ``solver`` == 'sag' or 'saga' to shuffle the data. See :term:`Glossary <random_state>` for details.  .. versionadded:: 0.17    `random_state` to support Stochastic Average Gradient."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,) or (n_targets, n_features)",
        "description": "Weight vector(s)."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``."
      },
      {
        "name": "n_iter_",
        "type": "None or ndarray of shape (n_targets,)",
        "description": "Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.  .. versionadded:: 0.17"
      }
    ]
  },
  "RidgeCV": {
    "name": "sklearn.linear_model._ridge.RidgeCV",
    "common_name": "RidgeCV",
    "description": "Ridge regression with built-in cross-validation. See glossary entry for :term:`cross-validation estimator`.  By default, it performs Leave-One-Out Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alphas",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "_structural_type": "tuple",
          "default": "&esc(0.1, 1.0, 10.0)",
          "description": "Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If using Leave-One-Out cross-validation, alphas must be positive."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "scoring",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "scoring"
          ],
          "default": "scoring__None",
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If None, the negative mean squared error if cv is 'auto' or None (i.e. when using leave-one-out cross-validation), and r2 score otherwise."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "scoring__str",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "scoring__Callable",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "Callable"
            }
          },
          {
            "type": "Constant",
            "name": "scoring__None",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if ``y`` is binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used, else, :class:`~sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "gcv_mode",
        "init_args": {
          "semantic_types": [
            "gcv_mode"
          ],
          "values": [
            "auto",
            "svd",
            "eigen"
          ],
          "_structural_type": "str",
          "default": "None",
          "description": "Flag indicating which strategy to use when performing Leave-One-Out Cross-Validation. Options are::      'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'     'svd' : force use of singular value decomposition of X when X is         dense, eigenvalue decomposition of X^T.X when X is sparse.     'eigen' : force computation via eigendecomposition of X.X^T  The 'auto' mode is the default and is intended to pick the cheaper option of the two depending on the shape of the training data."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "store_cv_values",
        "init_args": {
          "semantic_types": [
            "store_cv_values"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_values_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha_per_target",
        "init_args": {
          "semantic_types": [
            "alpha_per_target"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Flag indicating whether to optimize the alpha value (picked from the `alphas` parameter list) for each target separately (for multi-output settings: multiple prediction targets). When set to `True`, after fitting, the `alpha_` attribute will contain a value for each target. When set to `False`, a single alpha is used for all targets.  .. versionadded:: 0.24"
        }
      }
    ],
    "Params": [
      {
        "name": "cv_values_",
        "type": "ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional",
        "description": "Cross-validation values for each alpha (only available if ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been called, this attribute will contain the mean squared errors (by default) or the values of the ``{loss,score}_func`` function (if provided in the constructor)."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features) or (n_targets, n_features)",
        "description": "Weight vector(s)."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``."
      },
      {
        "name": "alpha_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Estimated regularization parameter, or, if ``alpha_per_target=True``, the estimated regularization parameter for each target."
      },
      {
        "name": "best_score_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Score of base estimator with best alpha, or, if ``alpha_per_target=True``, a score for each target.  .. versionadded:: 0.23"
      }
    ]
  },
  "RidgeClassifier": {
    "name": "sklearn.linear_model._ridge.RidgeClassifier",
    "common_name": "RidgeClassifier",
    "description": "Classifier using Ridge regression. This classifier first converts the target values into ``{-1, 1}`` and then treats the problem as a regression task (multi-output regression in the multiclass case).  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "name": "max_iter",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "default": "max_iter__None",
          "description": "Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "max_iter__int",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "max_iter__None",
            "init_args": {
              "semantic_types": [
                "max_iter"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Precision of the solution."
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "solver",
        "init_args": {
          "semantic_types": [
            "solver"
          ],
          "values": [
            "auto",
            "svd",
            "cholesky",
            "lsqr",
            "sparse_cg",
            "sag",
            "saga"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge   coefficients. More stable for singular matrices than 'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to   obtain a closed-form solution.  - 'sparse_cg' uses the conjugate gradient solver as found in   scipy.sparse.linalg.cg. As an iterative algorithm, this solver is   more appropriate than 'cholesky' for large-scale data   (possibility to set `tol` and `max_iter`).  - 'lsqr' uses the dedicated regularized least-squares routine   scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative   procedure.  - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses   its unbiased and more flexible version named SAGA. Both methods   use an iterative procedure, and are often faster than other solvers   when both n_samples and n_features are large. Note that 'sag' and   'saga' fast convergence is only guaranteed on features with   approximately the same scale. You can preprocess the data with a   scaler from sklearn.preprocessing.    .. versionadded:: 0.17      Stochastic Average Gradient descent solver.   .. versionadded:: 0.19    SAGA solver."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Used when ``solver`` == 'sag' or 'saga' to shuffle the data. See :term:`Glossary <random_state>` for details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (1, n_features) or (n_classes, n_features)",
        "description": "Coefficient of the features in the decision function.  ``coef_`` is of shape (1, n_features) when the given problem is binary."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``."
      },
      {
        "name": "n_iter_",
        "type": "None or ndarray of shape (n_targets,)",
        "description": "Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None."
      },
      {
        "name": "classes_",
        "type": "ndarray of shape (n_classes,)",
        "description": "The classes labels."
      }
    ]
  },
  "RidgeClassifierCV": {
    "name": "sklearn.linear_model._ridge.RidgeClassifierCV",
    "common_name": "RidgeClassifierCV",
    "description": "Ridge classifier with built-in cross-validation. See glossary entry for :term:`cross-validation estimator`.  By default, it performs Leave-One-Out Cross-Validation. Currently, only the n_features > n_samples case is handled efficiently.  Read more in the :ref:`User Guide <ridge_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "alphas",
        "init_args": {
          "semantic_types": [
            "alphas"
          ],
          "_structural_type": "tuple",
          "default": "&esc(0.1, 1.0, 10.0)",
          "description": "Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize",
        "init_args": {
          "semantic_types": [
            "normalize"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``."
        }
      },
      {
        "name": "scoring",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "scoring"
          ],
          "default": "None",
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "scoring__str",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "scoring__Callable",
            "init_args": {
              "semantic_types": [
                "scoring"
              ],
              "_structural_type": "Callable"
            }
          }
        ]
      },
      {
        "name": "cv",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "cv"
          ],
          "default": "cv__None",
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "cv__int",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "cv__None",
            "init_args": {
              "semantic_types": [
                "cv"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "store_cv_values",
        "init_args": {
          "semantic_types": [
            "store_cv_values"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_values_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation)."
        }
      }
    ],
    "Params": [
      {
        "name": "cv_values_",
        "type": "ndarray of shape (n_samples, n_targets, n_alphas), optional",
        "description": "Cross-validation values for each alpha (if ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been called, this attribute will contain the mean squared errors (by default) or the values of the ``{loss,score}_func`` function (if provided in the constructor). This attribute exists only when ``store_cv_values`` is True."
      },
      {
        "name": "coef_",
        "type": "ndarray of shape (1, n_features) or (n_targets, n_features)",
        "description": "Coefficient of the features in the decision function.  ``coef_`` is of shape (1, n_features) when the given problem is binary."
      },
      {
        "name": "intercept_",
        "type": "float or ndarray of shape (n_targets,)",
        "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``."
      },
      {
        "name": "alpha_",
        "type": "float",
        "description": "Estimated regularization parameter."
      },
      {
        "name": "best_score_",
        "type": "float",
        "description": "Score of base estimator with best alpha.  .. versionadded:: 0.23"
      },
      {
        "name": "classes_",
        "type": "ndarray of shape (n_classes,)",
        "description": "The classes labels."
      }
    ]
  },
  "SGDClassifier": {
    "name": "sklearn.linear_model._stochastic_gradient.SGDClassifier",
    "common_name": "SGDClassifier",
    "description": "Linear classifiers (SVM, logistic regression, etc.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the `partial_fit` method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.  This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  Read more in the :ref:`User Guide <sgd>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "hinge",
          "description": "The loss function to be used. Defaults to 'hinge', which gives a linear SVM.  The possible options are 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', or a regression loss: 'squared_loss', 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.  The 'log' loss gives logistic regression, a probabilistic classifier. 'modified_huber' is another smooth loss that brings tolerance to outliers as well as probability estimates. 'squared_hinge' is like hinge but is quadratically penalized. 'perceptron' is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see :class:`~sklearn.linear_model.SGDRegressor` for a description.  More details about the losses formulas can be found in the :ref:`User Guide <sgd_mathematical_formulation>`."
        }
      },
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l2",
            "l1",
            "elasticnet"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to `learning_rate` is set to 'optimal'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.15,
          "description": "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if `penalty` is 'elasticnet'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, training will stop when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive epochs.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For 'huber', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Used for shuffling the data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "learning_rate",
        "init_args": {
          "semantic_types": [
            "learning_rate"
          ],
          "values": [
            "constant",
            "optimal",
            "invscaling",
            "adaptive"
          ],
          "_structural_type": "str",
          "default": "optimal",
          "description": "The learning rate schedule:  - 'constant': `eta = eta0` - 'optimal': `eta = 1.0 / (alpha * (t + t0))`   where t0 is chosen by a heuristic proposed by Leon Bottou. - 'invscaling': `eta = eta0 / pow(t, power_t)` - 'adaptive': eta = eta0, as long as the training keeps decreasing.   Each time n_iter_no_change consecutive epochs fail to decrease the   training loss by tol or fail to increase validation score by tol if   early_stopping is True, the current learning rate is divided by 5.      .. versionadded:: 0.20         Added 'adaptive' option"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eta0",
        "init_args": {
          "semantic_types": [
            "eta0"
          ],
          "_structural_type": "float",
          "default": 0.0,
          "description": "The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules. The default value is 0.0 as eta0 is not used by the default schedule 'optimal'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "power_t",
        "init_args": {
          "semantic_types": [
            "power_t"
          ],
          "_structural_type": "float",
          "default": 0.5,
          "description": "The exponent for inverse scaling learning rate [default 0.5]."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score returned by the `score` method is not improving by at least tol for n_iter_no_change consecutive epochs.  .. versionadded:: 0.20     Added 'early_stopping' option"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if `early_stopping` is True.  .. versionadded:: 0.20     Added 'validation_fraction' option"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20     Added 'n_iter_no_change' option"
        }
      },
      {
        "name": "class_weight",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "class_weight"
          ],
          "default": "class_weight__None",
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "class_weight__dict",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "dict"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__str",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "str"
            }
          },
          {
            "type": "Constant",
            "name": "class_weight__None",
            "init_args": {
              "semantic_types": [
                "class_weight"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling ``fit`` resets this counter, while ``partial_fit`` will result in increasing the existing counter."
        }
      },
      {
        "name": "average",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "default": "average__bool",
          "description": "When set to True, computes the averaged SGD weights accross all updates and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches `average`. So ``average=10`` will begin averaging after seeing 10 samples."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "average__bool",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "average__int",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "int"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)",
        "description": "Weights assigned to the features."
      },
      {
        "name": "intercept_",
        "type": "ndarray of shape (1,) if n_classes == 2 else (n_classes,)",
        "description": "Constants in decision function."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "The actual number of iterations before reaching the stopping criterion. For multiclass fits, it is the maximum over every binary fit."
      },
      {
        "name": "loss_function_",
        "type": "concrete ``LossFunction``",
        "description": ""
      },
      {
        "name": "classes_",
        "type": "array of shape (n_classes,)",
        "description": ""
      },
      {
        "name": "t_",
        "type": "int",
        "description": "Number of weight updates performed during training. Same as ``(n_iter_ * n_samples)``."
      }
    ]
  },
  "SGDRegressor": {
    "name": "sklearn.linear_model._stochastic_gradient.SGDRegressor",
    "common_name": "SGDRegressor",
    "description": "Linear model fitted by minimizing a regularized empirical loss with SGD SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  This implementation works with data represented as dense numpy arrays of floating point values for the features.  Read more in the :ref:`User Guide <sgd>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "loss",
        "init_args": {
          "semantic_types": [
            "loss"
          ],
          "_structural_type": "str",
          "default": "squared_loss",
          "description": "The loss function to be used. The possible values are 'squared_loss', 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'  The 'squared_loss' refers to the ordinary least squares fit. 'huber' modifies 'squared_loss' to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. 'squared_epsilon_insensitive' is the same but becomes squared loss past a tolerance of epsilon.  More details about the losses formulas can be found in the :ref:`User Guide <sgd_mathematical_formulation>`."
        }
      },
      {
        "type": "Enumeration",
        "name": "penalty",
        "init_args": {
          "semantic_types": [
            "penalty"
          ],
          "values": [
            "l2",
            "l1",
            "elasticnet"
          ],
          "_structural_type": "str",
          "default": "l2",
          "description": "The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to `learning_rate` is set to 'optimal'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "l1_ratio",
        "init_args": {
          "semantic_types": [
            "l1_ratio"
          ],
          "_structural_type": "float",
          "default": 0.15,
          "description": "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if `penalty` is 'elasticnet'."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 1000,
          "description": "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "The stopping criterion. If it is not None, training will stop when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive epochs.  .. versionadded:: 0.19"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "shuffle",
        "init_args": {
          "semantic_types": [
            "shuffle"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether or not the training data should be shuffled after each epoch."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The verbosity level."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "epsilon",
        "init_args": {
          "semantic_types": [
            "epsilon"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For 'huber', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Used for shuffling the data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "learning_rate",
        "init_args": {
          "semantic_types": [
            "learning_rate"
          ],
          "values": [
            "constant",
            "optimal",
            "invscaling",
            "adaptive"
          ],
          "_structural_type": "str",
          "default": "invscaling",
          "description": "The learning rate schedule:  - 'constant': `eta = eta0` - 'optimal': `eta = 1.0 / (alpha * (t + t0))`   where t0 is chosen by a heuristic proposed by Leon Bottou. - 'invscaling': `eta = eta0 / pow(t, power_t)` - 'adaptive': eta = eta0, as long as the training keeps decreasing.   Each time n_iter_no_change consecutive epochs fail to decrease the   training loss by tol or fail to increase validation score by tol if   early_stopping is True, the current learning rate is divided by 5.      .. versionadded:: 0.20         Added 'adaptive' option"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "eta0",
        "init_args": {
          "semantic_types": [
            "eta0"
          ],
          "_structural_type": "float",
          "default": 0.01,
          "description": "The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules. The default value is 0.01."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "power_t",
        "init_args": {
          "semantic_types": [
            "power_t"
          ],
          "_structural_type": "float",
          "default": 0.25,
          "description": "The exponent for inverse scaling learning rate."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "early_stopping",
        "init_args": {
          "semantic_types": [
            "early_stopping"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score returned by the `score` method is not improving by at least `tol` for `n_iter_no_change` consecutive epochs.  .. versionadded:: 0.20     Added 'early_stopping' option"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "validation_fraction",
        "init_args": {
          "semantic_types": [
            "validation_fraction"
          ],
          "_structural_type": "float",
          "default": 0.1,
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if `early_stopping` is True.  .. versionadded:: 0.20     Added 'validation_fraction' option"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "n_iter_no_change",
        "init_args": {
          "semantic_types": [
            "n_iter_no_change"
          ],
          "_structural_type": "int",
          "default": 5,
          "description": "Number of iterations with no improvement to wait before early stopping.  .. versionadded:: 0.20     Added 'n_iter_no_change' option"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling ``fit`` resets this counter, while ``partial_fit``  will result in increasing the existing counter."
        }
      },
      {
        "name": "average",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "average"
          ],
          "default": "average__bool",
          "description": "When set to True, computes the averaged SGD weights accross all updates and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches `average`. So ``average=10`` will begin averaging after seeing 10 samples."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "average__bool",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "bool",
              "default": "False"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "average__int",
            "init_args": {
              "semantic_types": [
                "average"
              ],
              "_structural_type": "int"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,)",
        "description": "Weights assigned to the features."
      },
      {
        "name": "intercept_",
        "type": "ndarray of shape (1,)",
        "description": "The intercept term."
      },
      {
        "name": "average_coef_",
        "type": "ndarray of shape (n_features,)",
        "description": "Averaged weights assigned to the features. Only available if ``average=True``.  .. deprecated:: 0.23     Attribute ``average_coef_`` was deprecated     in version 0.23 and will be removed in 1.0 (renaming of 0.25)."
      },
      {
        "name": "average_intercept_",
        "type": "ndarray of shape (1,)",
        "description": "The averaged intercept term. Only available if ``average=True``.  .. deprecated:: 0.23     Attribute ``average_intercept_`` was deprecated     in version 0.23 and will be removed in 1.0 (renaming of 0.25)."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "The actual number of iterations before reaching the stopping criterion."
      },
      {
        "name": "t_",
        "type": "int",
        "description": "Number of weight updates performed during training. Same as ``(n_iter_ * n_samples)``."
      }
    ]
  },
  "TheilSenRegressor": {
    "name": "sklearn.linear_model._theil_sen.TheilSenRegressor",
    "common_name": "TheilSenRegressor",
    "description": "Theil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is \"n_samples choose n_subsamples\", it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.  Read more in the :ref:`User Guide <theil_sen_regression>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X",
        "init_args": {
          "semantic_types": [
            "copy_X"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, X will be copied; else, it may be overwritten."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_subpopulation",
        "init_args": {
          "semantic_types": [
            "max_subpopulation"
          ],
          "_structural_type": "int",
          "default": 10000.0,
          "description": "Instead of computing with a set of cardinality 'n choose k', where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if 'n choose k' is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed."
        }
      },
      {
        "name": "n_subsamples",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_subsamples"
          ],
          "default": "n_subsamples__None",
          "description": "Number of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_subsamples__int",
            "init_args": {
              "semantic_types": [
                "n_subsamples"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_subsamples__None",
            "init_args": {
              "semantic_types": [
                "n_subsamples"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 300,
          "description": "Maximum number of iterations for the calculation of spatial median."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.001,
          "description": "Tolerance when calculating spatial median."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "A random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`"
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Verbose mode when fitting the model."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "ndarray of shape (n_features,)",
        "description": "Coefficients of the regression model (median of distribution)."
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "Estimated intercept of regression model."
      },
      {
        "name": "breakdown_",
        "type": "float",
        "description": "Approximated breakdown point."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Number of iterations needed for the spatial median."
      },
      {
        "name": "n_subpopulation_",
        "type": "int",
        "description": "Number of combinations taken into account from 'n choose k', where n is the number of samples and k is the number of subsamples."
      }
    ]
  },
  "TweedieRegressor": {
    "name": "sklearn.linear_model._glm.glm.TweedieRegressor",
    "common_name": "TweedieRegressor",
    "description": "Generalized Linear Model with a Tweedie distribution. This estimator can be used to model different GLMs depending on the ``power`` parameter, which determines the underlying distribution.  Read more in the :ref:`User Guide <Generalized_linear_regression>`.  .. versionadded:: 0.23",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "type": "Hyperparameter",
        "name": "power",
        "init_args": {
          "semantic_types": [
            "power"
          ],
          "_structural_type": "float",
          "default": 0.0,
          "description": "The power determines the underlying target distribution according to the following table:  +-------+------------------------+ | Power | Distribution           | +=======+========================+ | 0     | Normal                 | +-------+------------------------+ | 1     | Poisson                | +-------+------------------------+ | (1,2) | Compound Poisson Gamma | +-------+------------------------+ | 2     | Gamma                  | +-------+------------------------+ | 3     | Inverse Gaussian       | +-------+------------------------+  For ``0 < power < 1``, no distribution exists."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "alpha",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "_structural_type": "float",
          "default": 1.0,
          "description": "Constant that multiplies the penalty term and thus determines the regularization strength. ``alpha = 0`` is equivalent to unpenalized GLMs. In this case, the design matrix `X` must have full column rank (no collinearities)."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "fit_intercept",
        "init_args": {
          "semantic_types": [
            "fit_intercept"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (X @ coef + intercept)."
        }
      },
      {
        "type": "Enumeration",
        "name": "link",
        "init_args": {
          "semantic_types": [
            "link"
          ],
          "values": [
            "auto",
            "identity",
            "log"
          ],
          "_structural_type": "str",
          "default": "auto",
          "description": "The link function of the GLM, i.e. mapping from linear predictor `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets the link depending on the chosen family as follows:  - 'identity' for Normal distribution - 'log' for Poisson,  Gamma and Inverse Gaussian distributions"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter",
        "init_args": {
          "semantic_types": [
            "max_iter"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "The maximal number of iterations for the solver."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "tol",
        "init_args": {
          "semantic_types": [
            "tol"
          ],
          "_structural_type": "float",
          "default": 0.0001,
          "description": "Stopping criterion. For the lbfgs solver, the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol`` where ``g_j`` is the j-th component of the gradient (derivative) of the objective function."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If set to ``True``, reuse the solution of the previous call to ``fit`` as initialization for ``coef_`` and ``intercept_`` ."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "verbose",
        "init_args": {
          "semantic_types": [
            "verbose"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "For the lbfgs solver set verbose to any positive number for verbosity."
        }
      }
    ],
    "Params": [
      {
        "name": "coef_",
        "type": "array of shape (n_features,)",
        "description": "Estimated coefficients for the linear predictor (`X @ coef_ + intercept_`) in the GLM."
      },
      {
        "name": "intercept_",
        "type": "float",
        "description": "Intercept (a.k.a. bias) added to linear predictor."
      },
      {
        "name": "n_iter_",
        "type": "int",
        "description": "Actual number of iterations used in the solver."
      }
    ]
  }
}