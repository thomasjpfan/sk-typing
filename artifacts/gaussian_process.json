{
  "GaussianProcessClassifier": {
    "name": "sklearn.gaussian_process._gpc.GaussianProcessClassifier",
    "common_name": "GaussianProcessClassifier",
    "description": "Gaussian process classification (GPC) based on Laplace approximation. The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.  Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.  Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.  Read more in the :ref:`User Guide <gaussian_process>`.",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "kernel",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "kernel"
          ],
          "default": "kernel__None",
          "description": "The kernel specifying the covariance function of the GP. If None is passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that the kernel's hyperparameters are optimized during fitting."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "kernel__None",
            "init_args": {
              "semantic_types": [
                "kernel"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "optimizer",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "optimizer"
          ],
          "default": "optimizer__str",
          "description": "Can either be one of the internally supported optimizers for optimizing the kernel's parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the  signature::      def optimizer(obj_func, initial_theta, bounds):         # * 'obj_func' is the objective function to be maximized, which         #   takes the hyperparameters theta as parameter and an         #   optional flag eval_gradient, which determines if the         #   gradient is returned additionally to the function value         # * 'initial_theta': the initial value for theta, which can be         #   used by local optimizers         # * 'bounds': the bounds on the values of theta         ....         # Returned are the best found hyperparameters theta and         # the corresponding value of the target function.         return theta_opt, func_min  Per default, the 'L-BFGS-B' algorithm from scipy.optimize.minimize is used. If None is passed, the kernel's parameters are kept fixed. Available internal optimizers are::      'fmin_l_bfgs_b'"
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "optimizer__str",
            "init_args": {
              "semantic_types": [
                "optimizer"
              ],
              "_structural_type": "str",
              "default": "fmin_l_bfgs_b"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "optimizer__Callable",
            "init_args": {
              "semantic_types": [
                "optimizer"
              ],
              "_structural_type": "Callable"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "n_restarts_optimizer",
        "init_args": {
          "semantic_types": [
            "n_restarts_optimizer"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The number of restarts of the optimizer for finding the kernel's parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel's initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "max_iter_predict",
        "init_args": {
          "semantic_types": [
            "max_iter_predict"
          ],
          "_structural_type": "int",
          "default": 100,
          "description": "The maximum number of iterations in Newton's method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "warm_start",
        "init_args": {
          "semantic_types": [
            "warm_start"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See :term:`the Glossary <warm_start>`."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X_train",
        "init_args": {
          "semantic_types": [
            "copy_X_train"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Determines random number generation used to initialize the centers. Pass an int for reproducible results across multiple function calls. See :term: `Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "type": "Enumeration",
        "name": "multi_class",
        "init_args": {
          "semantic_types": [
            "multi_class"
          ],
          "values": [
            "one_vs_rest",
            "one_vs_one"
          ],
          "_structural_type": "str",
          "default": "one_vs_rest",
          "description": "Specifies how multi-class classification problems are handled. Supported are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest', one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In 'one_vs_one', one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. Note that 'one_vs_one' does not support predicting probability estimates."
        }
      },
      {
        "name": "n_jobs",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "n_jobs"
          ],
          "default": "n_jobs__None",
          "description": "The number of jobs to use for the computation: the specified multiclass problems are computed in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "n_jobs__int",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "n_jobs__None",
            "init_args": {
              "semantic_types": [
                "n_jobs"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "base_estimator_",
        "type": "``Estimator`` instance",
        "description": "The estimator instance that defines the likelihood function using the observed data."
      },
      {
        "name": "kernel_",
        "type": "kernel instance",
        "description": "The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers."
      },
      {
        "name": "log_marginal_likelihood_value_",
        "type": "float",
        "description": "The log-marginal-likelihood of ``self.kernel_.theta``"
      },
      {
        "name": "classes_",
        "type": "array-like of shape (n_classes,)",
        "description": "Unique class labels."
      },
      {
        "name": "n_classes_",
        "type": "int",
        "description": "The number of classes in the training data"
      }
    ]
  },
  "GaussianProcessRegressor": {
    "name": "sklearn.gaussian_process._gpr.GaussianProcessRegressor",
    "common_name": "GaussianProcessRegressor",
    "description": "Gaussian process regression (GPR). The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.  In addition to standard scikit-learn estimator API, GaussianProcessRegressor:     * allows prediction without prior fitting (based on the GP prior)    * provides an additional method sample_y(X), which evaluates samples      drawn from the GPR (prior or posterior) at given inputs    * exposes a method log_marginal_likelihood(theta), which can be used      externally for other ways of selecting hyperparameters, e.g., via      Markov chain Monte Carlo.  Read more in the :ref:`User Guide <gaussian_process>`.  .. versionadded:: 0.18",
    "sklearn_version": "0.24.0",
    "Hyperparams": [
      {
        "name": "kernel",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "kernel"
          ],
          "default": "kernel__None",
          "description": "The kernel specifying the covariance function of the GP. If None is passed, the kernel ``ConstantKernel(1.0, constant_value_bounds=\"fixed\" * RBF(1.0, length_scale_bounds=\"fixed\")`` is used as default. Note that the kernel hyperparameters are optimized during fitting unless the bounds are marked as \"fixed\"."
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "kernel__None",
            "init_args": {
              "semantic_types": [
                "kernel"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      },
      {
        "name": "alpha",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "alpha"
          ],
          "default": "alpha__float",
          "description": "Value added to the diagonal of the kernel matrix during fitting. This can prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. It can also be interpreted as the variance of additional Gaussian measurement noise on the training observations. Note that this is different from using a `WhiteKernel`. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "alpha__float",
            "init_args": {
              "semantic_types": [
                "alpha"
              ],
              "_structural_type": "float",
              "default": 1e-10
            }
          },
          {
            "name": "alpha__ndarray",
            "type": "Hyperparameter",
            "init_args": {
              "_structural_type": "ndarray",
              "semantic_types": [
                "alpha"
              ]
            }
          }
        ]
      },
      {
        "name": "optimizer",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "optimizer"
          ],
          "default": "optimizer__str",
          "description": "Can either be one of the internally supported optimizers for optimizing the kernel's parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the signature::      def optimizer(obj_func, initial_theta, bounds):         # * 'obj_func' is the objective function to be minimized, which         #   takes the hyperparameters theta as parameter and an         #   optional flag eval_gradient, which determines if the         #   gradient is returned additionally to the function value         # * 'initial_theta': the initial value for theta, which can be         #   used by local optimizers         # * 'bounds': the bounds on the values of theta         ....         # Returned are the best found hyperparameters theta and         # the corresponding value of the target function.         return theta_opt, func_min  Per default, the 'L-BGFS-B' algorithm from scipy.optimize.minimize is used. If None is passed, the kernel's parameters are kept fixed. Available internal optimizers are::      'fmin_l_bfgs_b'"
        },
        "hyperparams": [
          {
            "type": "Constant",
            "name": "optimizer__str",
            "init_args": {
              "semantic_types": [
                "optimizer"
              ],
              "_structural_type": "str",
              "default": "fmin_l_bfgs_b"
            }
          },
          {
            "type": "Hyperparameter",
            "name": "optimizer__Callable",
            "init_args": {
              "semantic_types": [
                "optimizer"
              ],
              "_structural_type": "Callable"
            }
          }
        ]
      },
      {
        "type": "Hyperparameter",
        "name": "n_restarts_optimizer",
        "init_args": {
          "semantic_types": [
            "n_restarts_optimizer"
          ],
          "_structural_type": "int",
          "default": 0,
          "description": "The number of restarts of the optimizer for finding the kernel's parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel's initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed."
        }
      },
      {
        "type": "Hyperparameter",
        "name": "normalize_y",
        "init_args": {
          "semantic_types": [
            "normalize_y"
          ],
          "_structural_type": "bool",
          "default": "False",
          "description": "Whether the target values y are normalized, the mean and variance of the target values are set equal to 0 and 1 respectively. This is recommended for cases where zero-mean, unit-variance priors are used. Note that, in this implementation, the normalisation is reversed before the GP predictions are reported.  .. versionchanged:: 0.23"
        }
      },
      {
        "type": "Hyperparameter",
        "name": "copy_X_train",
        "init_args": {
          "semantic_types": [
            "copy_X_train"
          ],
          "_structural_type": "bool",
          "default": "True",
          "description": "If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally."
        }
      },
      {
        "name": "random_state",
        "type": "Union",
        "init_args": {
          "semantic_types": [
            "random_state"
          ],
          "default": "random_state__None",
          "description": "Determines random number generation used to initialize the centers. Pass an int for reproducible results across multiple function calls. See :term: `Glossary <random_state>`."
        },
        "hyperparams": [
          {
            "type": "Hyperparameter",
            "name": "random_state__int",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "int"
            }
          },
          {
            "type": "Constant",
            "name": "random_state__None",
            "init_args": {
              "semantic_types": [
                "random_state"
              ],
              "_structural_type": "None",
              "default": "None"
            }
          }
        ]
      }
    ],
    "Params": [
      {
        "name": "X_train_",
        "type": "array-like of shape (n_samples, n_features) or list of object",
        "description": "Feature vectors or other representations of training data (also required for prediction)."
      },
      {
        "name": "y_train_",
        "type": "array-like of shape (n_samples,) or (n_samples, n_targets)",
        "description": "Target values in training data (also required for prediction)"
      },
      {
        "name": "kernel_",
        "type": "kernel instance",
        "description": "The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters"
      },
      {
        "name": "L_",
        "type": "array-like of shape (n_samples, n_samples)",
        "description": "Lower-triangular Cholesky decomposition of the kernel in ``X_train_``"
      },
      {
        "name": "alpha_",
        "type": "array-like of shape (n_samples,)",
        "description": "Dual coefficients of training data points in kernel space"
      },
      {
        "name": "log_marginal_likelihood_value_",
        "type": "float",
        "description": "The log-marginal-likelihood of ``self.kernel_.theta``"
      }
    ]
  }
}